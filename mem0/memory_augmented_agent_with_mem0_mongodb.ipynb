{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "04323d1d",
      "metadata": {},
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/binchow-ai/agent_memory_demo/blob/main/mem0/memory_augmented_agent_with_mem0_mongodb.ipynb)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "49e1ccaa",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Looking in indexes: https://pypi.org/simple/\n",
            "Requirement already satisfied: pip in /Users/binzhou/Demo/agent_memory_demo/venv/lib/python3.12/site-packages (26.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install -qU mem0ai azure-identity langchain_voyageai pymongo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "9b701237",
      "metadata": {},
      "outputs": [],
      "source": [
        "# å¸¸é‡\n",
        "CURRENT_USER_ID = \"user-123\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9bab6374",
      "metadata": {},
      "outputs": [],
      "source": [
        "# import getpass\n",
        "# import os\n",
        "\n",
        "# try:\n",
        "#     from dotenv import load_dotenv\n",
        "#     load_dotenv()\n",
        "# except ImportError:\n",
        "#     pass  # æ—  python-dotenv æ—¶ä½¿ç”¨ç³»ç»Ÿç¯å¢ƒå˜é‡\n",
        "\n",
        "# # å®‰å…¨åœ°è·å–å¹¶è®¾ç½®ç¯å¢ƒå˜é‡çš„å‡½æ•°\n",
        "# def set_env_securely(var_name, prompt):\n",
        "#     value = getpass.getpass(prompt)\n",
        "#     os.environ[var_name] = value\n",
        "\n",
        "# set_env_securely(\"OPENAI_API_KEY\", \"Enter your OPENAI_API_KEY: \")\n",
        "# set_env_securely(\"MONGODB_URI\", \"Enter your MongoDB URI: \")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c8a7f177",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "from langchain_voyageai import VoyageAIEmbeddings\n",
        "import os   \n",
        "\n",
        "config = {\n",
        "    \"vector_store\": {\n",
        "        \"provider\": \"mongodb\",\n",
        "        \"config\": {\n",
        "            \"db_name\": \"mem0_agent_memory\",\n",
        "            \"collection_name\": \"extracted_memories\",\n",
        "            \"mongo_uri\": os.environ[\"MONGODB_URI\"],\n",
        "            \"embedding_model_dims\": 1024\n",
        "        }\n",
        "    },\n",
        "    \"embedder\": {\n",
        "        \"provider\": \"langchain\",\n",
        "        \"config\": {\n",
        "            \"model\": VoyageAIEmbeddings(\n",
        "                model=\"voyage-4-large\",\n",
        "                voyage_api_key=os.getenv(\"VOYAGE_API_KEY\")\n",
        "            ),\n",
        "            \"embedding_dims\": 1024\n",
        "        }\n",
        "    },\n",
        "    \"llm\": {\n",
        "        \"provider\": \"azure_openai\",\n",
        "        \"config\": {\n",
        "            \"model\": \"gpt-4o\",\n",
        "            \"azure_kwargs\": {\n",
        "                \"api_key\": os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
        "                \"azure_endpoint\": os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
        "                \"azure_deployment\": \"gpt-4o\",\n",
        "                \"api_version\": \"2025-03-01-preview\"\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "# semantic_memory_config = {\n",
        "#     \"vector_store\": {\n",
        "#         \"provider\": \"mongodb\",\n",
        "#         \"config\": {\n",
        "#             \"db_name\": \"mem0_agent_memory\",\n",
        "#             \"collection_name\": \"semantic_memory\",\n",
        "#             \"mongo_uri\": os.environ[\"MONGODB_URI\"]\n",
        "#         }\n",
        "#     }\n",
        "# }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "02f078ef",
      "metadata": {},
      "outputs": [],
      "source": [
        "# åœ¨ semantic_memory_configï¼ˆä»¥åŠéœ€è¦ç”¨åˆ° Voyage çš„ configï¼‰é‡Œï¼š\n",
        "semantic_memory_config = {\n",
        "    \"vector_store\": {\n",
        "        \"provider\": \"mongodb\",\n",
        "        \"config\": {\n",
        "            \"db_name\": \"mem0_agent_memory\",\n",
        "            \"collection_name\": \"semantic_memory\",\n",
        "            \"mongo_uri\": os.environ[\"MONGODB_URI\"],\n",
        "            \"embedding_model_dims\": 1024\n",
        "        }\n",
        "    },\n",
        "    \"embedder\": {\n",
        "        \"provider\": \"langchain\",\n",
        "        \"config\": {\n",
        "            \"model\": VoyageAIEmbeddings(\n",
        "                model=\"voyage-4-large\",\n",
        "                voyage_api_key=os.getenv(\"VOYAGE_API_KEY\")\n",
        "            ),\n",
        "            \"embedding_dims\": 1024\n",
        "        }\n",
        "    }\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6230bb8c",
      "metadata": {},
      "outputs": [],
      "source": [
        "# è‹¥æ›¾ç”¨ 1536 ç»´ï¼ˆå¦‚ OpenAIï¼‰å»ºè¿‡ç´¢å¼•ï¼Œéœ€å…ˆåˆ æ‰æ—§ç´¢å¼•ï¼Œå†è·‘ä¸‹é¢ Memory.from_config ä¼šæŒ‰ 1024 ç»´ï¼ˆVoyageï¼‰é‡å»ºã€‚\n",
        "# åªéœ€åœ¨å‡ºç° \"vector field is indexed with 1536 dimensions but queried with 1024\" æ—¶è¿è¡Œä¸€æ¬¡å³å¯ã€‚\n",
        "from pymongo import MongoClient\n",
        "_client = MongoClient(os.environ[\"MONGODB_URI\"])\n",
        "_db = _client[\"mem0_agent_memory\"]\n",
        "for _coll_name in [\"extracted_memories\", \"semantic_memory\"]:\n",
        "    _idx_name = f\"{_coll_name}_vector_index\"\n",
        "    try:\n",
        "        _db[_coll_name].drop_search_index(_idx_name)\n",
        "        print(f\"Dropped search index '{_idx_name}'.\")\n",
        "    except Exception as e:\n",
        "        if \"index not found\" in str(e).lower() or \"no such index\" in str(e).lower():\n",
        "            print(f\"Index '{_idx_name}' not found (ok).\")\n",
        "        else:\n",
        "            print(f\"Drop '{_idx_name}': {e}\")\n",
        "_client.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "03665250",
      "metadata": {},
      "outputs": [],
      "source": [
        "# è‹¥æŠ¥é”™ \"Index 'extracted_memories_vector_index' does not exist\"ï¼Œå…ˆè¿è¡Œæœ¬ cell åˆ›å»ºç´¢å¼•ï¼Œå†é‡æ–°è¿è¡Œä¸‹é¢çš„ Memory.from_config\n",
        "from pymongo import MongoClient\n",
        "from pymongo.operations import SearchIndexModel\n",
        "\n",
        "_uri = os.environ.get(\"MONGODB_URI\")\n",
        "if _uri:\n",
        "    _c = MongoClient(_uri)\n",
        "    _db = _c[\"mem0_agent_memory\"]\n",
        "    _dims = 1024\n",
        "    for _coll_name in [\"extracted_memories\", \"semantic_memory\"]:\n",
        "        _col = _db[_coll_name]\n",
        "        _idx_name = f\"{_coll_name}_vector_index\"\n",
        "        _existing = list(_col.list_search_indexes(name=_idx_name))\n",
        "        if _existing:\n",
        "            print(f\"Index '{_idx_name}' already exists.\")\n",
        "        else:\n",
        "            _col.create_search_index(SearchIndexModel(\n",
        "                name=_idx_name,\n",
        "                definition={\n",
        "                    \"mappings\": {\n",
        "                        \"dynamic\": False,\n",
        "                        \"fields\": {\n",
        "                            \"embedding\": {\"type\": \"knnVector\", \"dimensions\": _dims, \"similarity\": \"cosine\"}\n",
        "                        }\n",
        "                    }\n",
        "                }\n",
        "            ))\n",
        "            print(f\"Created index '{_idx_name}' (may take a minute on Atlas).\")\n",
        "    _c.close()\n",
        "else:\n",
        "    print(\"MONGODB_URI not set.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "805e0f8e",
      "metadata": {},
      "outputs": [],
      "source": [
        "from mem0 import Memory\n",
        "memory = Memory.from_config(config)\n",
        "semantic_memory = Memory.from_config(semantic_memory_config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7dce7b40",
      "metadata": {},
      "outputs": [],
      "source": [
        "from typing import Optional, Dict\n",
        "\n",
        "# ä½¿ç”¨ Azure æ—¶åŠ ä¸Šè¿™è¡Œï¼Œå¦åˆ™ä¼šèµ°åˆ° Azure çš„ tracing ç³»ç»Ÿï¼ŒæŠ¥401Error\n",
        "os.environ[\"OPENAI_AGENTS_DISABLE_TRACING\"] = \"true\"\n",
        "\n",
        "# å¯¼å…¥ Azure OpenAI Python å®¢æˆ·ç«¯åº“\n",
        "from openai import AzureOpenAI\n",
        "\n",
        "# åˆå§‹åŒ– OpenAI å®¢æˆ·ç«¯\n",
        "# å°†ä½¿ç”¨ç¯å¢ƒå˜é‡ OPENAI_API_KEY ä¸­çš„ API å¯†é’¥\n",
        "#openai_client = OpenAI()\n",
        "openai_client = AzureOpenAI(\n",
        "    api_version=\"2025-03-01-preview\",\n",
        "    azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
        "    api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
        "    azure_deployment=\"gpt-4o\",\n",
        ")\n",
        "\n",
        "\n",
        "def patched_search_vector_store(self, query, filters, limit, threshold: Optional[float] = None):\n",
        "    \"\"\"\n",
        "    ä½¿ç”¨æ­£ç¡® MongoDB å‚æ•°åçš„ _search_vector_store è¡¥ä¸ç‰ˆæœ¬ã€‚\n",
        "    \n",
        "    mem0 MongoDB search ç­¾åä¸º (query, vectors, limit, filters)ï¼Œå‚æ•°åä¸º vectorsã€‚\n",
        "    \"\"\"\n",
        "    if not query or (isinstance(query, str) and not query.strip()):\n",
        "        return []\n",
        "    # ä¸ºæŸ¥è¯¢ç”ŸæˆåµŒå…¥å‘é‡\n",
        "    embeddings = self.embedding_model.embed(query, \"search\")\n",
        "\n",
        "    # Call MongoDB search with correct parameter name: query_vector instead of vectors\n",
        "    memories = self.vector_store.search(query=query, vectors=embeddings, limit=limit, filters=filters)\n",
        "    \n",
        "    # å°†ç»“æœå¤„ç†ä¸ºæœŸæœ›çš„æ ¼å¼\n",
        "    promoted_payload_keys = [\n",
        "        \"user_id\",\n",
        "        \"agent_id\", \n",
        "        \"run_id\",\n",
        "        \"actor_id\",\n",
        "        \"role\",\n",
        "    ]\n",
        "\n",
        "    core_and_promoted_keys = {\"data\", \"hash\", \"created_at\", \"updated_at\", \"id\", *promoted_payload_keys}\n",
        "\n",
        "    # è‹¥æŒ‡å®šåˆ™åº”ç”¨é˜ˆå€¼è¿‡æ»¤\n",
        "    if threshold is not None:\n",
        "        filtered_memories = []\n",
        "        for memory_item in memories:\n",
        "            if hasattr(memory_item, 'score') and memory_item.score >= threshold:\n",
        "                filtered_memories.append(memory_item)\n",
        "        memories = filtered_memories\n",
        "\n",
        "    # å°†è®°å¿†åºåˆ—åŒ–ä¸ºæœŸæœ›çš„æ ¼å¼\n",
        "    serialized_memories = []\n",
        "    for memory_item in memories:\n",
        "        serialized_memory = {}\n",
        "        \n",
        "        # å¤„ç†æ ¸å¿ƒå­—æ®µ\n",
        "        for key in core_and_promoted_keys:\n",
        "            if key == \"data\":\n",
        "                # å®é™…è®°å¿†å†…å®¹åœ¨ payload ä¸­\n",
        "                serialized_memory[key] = getattr(memory_item, 'payload', {}).get('data', None)\n",
        "            else:\n",
        "                serialized_memory[key] = getattr(memory_item, key, None)\n",
        "\n",
        "        # æ·»åŠ åˆ†æ•°å’Œè®°å¿†å†…å®¹\n",
        "        serialized_memory[\"score\"] = getattr(memory_item, \"score\", None)\n",
        "        serialized_memory[\"memory\"] = getattr(memory_item, 'payload', {}).get('data', '')\n",
        "        serialized_memory[\"metadata\"] = getattr(memory_item, 'payload', {}).get('metadata', {})\n",
        "\n",
        "        # æ·»åŠ æå‡çš„ payload é”®\n",
        "        payload = getattr(memory_item, 'payload', {})\n",
        "        for key in promoted_payload_keys:\n",
        "            value = payload.get(key, None)\n",
        "            if value is not None:\n",
        "                serialized_memory[key] = value\n",
        "\n",
        "        serialized_memories.append(serialized_memory)\n",
        "\n",
        "    return serialized_memories\n",
        "\n",
        "def create_patched_add_to_vector_store(original_method):\n",
        "    \"\"\"\n",
        "    å·¥å‚å‡½æ•°ï¼Œç”¨äºåˆ›å»ºè¡¥ä¸ç‰ˆçš„ _add_to_vector_store æ–¹æ³•ã€‚\n",
        "    ç¡®ä¿æ¯ä¸ªè®°å¿†å®ä¾‹éƒ½æœ‰è‡ªå·±å¯¹åŸå§‹æ–¹æ³•çš„å¼•ç”¨ã€‚\n",
        "    \"\"\"\n",
        "    def patched_add_to_vector_store(self, messages, metadata, filters, infer):\n",
        "        \"\"\"\n",
        "        ä¿®å¤ MongoDB æœç´¢å‚æ•°é—®é¢˜çš„ _add_to_vector_store è¡¥ä¸ç‰ˆæœ¬ã€‚\n",
        "        \n",
        "        è¯¥æ–¹æ³•åœ¨ memory.add() æ“ä½œæœŸé—´è¢«è°ƒç”¨ï¼Œæ­¤å‰ä¹Ÿé”™è¯¯åœ°ä½¿ç”¨äº† 'vectors' å‚æ•°ã€‚\n",
        "        \"\"\"\n",
        "        # ä»åŸå§‹æ¨¡å—å¯¼å…¥ logger\n",
        "        import logging\n",
        "        logger = logging.getLogger(__name__)\n",
        "        \n",
        "        # è·å–å®é™…çš„ MongoDB å‘é‡å­˜å‚¨æœç´¢æ–¹æ³•ï¼ˆéæˆ‘ä»¬çš„è¡¥ä¸ç‰ˆæœ¬ï¼‰\n",
        "        mongodb_vector_store = self.vector_store\n",
        "        \n",
        "        # ç›´æ¥è·å–åŸå§‹ MongoDB æœç´¢æ–¹æ³•\n",
        "        original_mongodb_search = mongodb_vector_store.__class__.search\n",
        "        \n",
        "        def patched_vector_store_search(self_vs, query, vectors, limit, filters):\n",
        "            return original_mongodb_search(self_vs, query=query, vectors=vectors, limit=limit, filters=filters)\n",
        "        \n",
        "        # ä¸´æ—¶è¡¥ä¸å‘é‡å­˜å‚¨çš„æœç´¢æ–¹æ³•\n",
        "        original_vector_store_search = mongodb_vector_store.search\n",
        "        mongodb_vector_store.search = patched_vector_store_search.__get__(mongodb_vector_store, mongodb_vector_store.__class__)\n",
        "        \n",
        "        try:\n",
        "            # ä»¥ç»‘å®šæ–¹æ³•å½¢å¼è°ƒç”¨åŸå§‹ _add_to_vector_store æ–¹æ³•\n",
        "            result = original_method.__get__(self, type(self))(messages, metadata, filters, infer)\n",
        "            return result\n",
        "        finally:\n",
        "            # æ¢å¤åŸå§‹å‘é‡å­˜å‚¨çš„æœç´¢æ–¹æ³•\n",
        "            mongodb_vector_store.search = original_vector_store_search\n",
        "    \n",
        "    return patched_add_to_vector_store"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1a4d047f",
      "metadata": {},
      "outputs": [],
      "source": [
        "# åœ¨ç±»å’Œå®ä¾‹ä¸¤ä¸ªå±‚çº§åº”ç”¨çŒ´å­è¡¥ä¸\n",
        "from mem0.memory.main import Memory\n",
        "\n",
        "# åœ¨æ‰“è¡¥ä¸ä¹‹å‰ä¿å­˜æ¯ä¸ªå®ä¾‹çš„åŸå§‹æ–¹æ³•\n",
        "memory_original_search = memory._search_vector_store\n",
        "memory_original_add = memory._add_to_vector_store\n",
        "semantic_memory_original_search = semantic_memory._search_vector_store\n",
        "semantic_memory_original_add = semantic_memory._add_to_vector_store\n",
        "\n",
        "# åœ¨ä»»ä½•è¡¥ä¸ä¹‹å‰ä¿å­˜åŸå§‹ç±»æ–¹æ³•\n",
        "original_class_search = Memory._search_vector_store\n",
        "original_class_add = Memory._add_to_vector_store\n",
        "\n",
        "# åº”ç”¨ç±»çº§åˆ«è¡¥ä¸ï¼ˆå½±å“æ–°å®ä¾‹ï¼‰\n",
        "Memory._search_vector_store = patched_search_vector_store\n",
        "Memory._add_to_vector_store = create_patched_add_to_vector_store(original_class_add)\n",
        "\n",
        "# ä¸ºå„å®ä¾‹åˆ›å»ºä¸“ç”¨çš„è¡¥ä¸ add æ–¹æ³•\n",
        "memory_patched_add = create_patched_add_to_vector_store(memory_original_add)\n",
        "semantic_memory_patched_add = create_patched_add_to_vector_store(semantic_memory_original_add)\n",
        "\n",
        "# å¯¹å·²æœ‰å®ä¾‹åº”ç”¨è¡¥ä¸\n",
        "memory._search_vector_store = patched_search_vector_store.__get__(memory, memory.__class__)\n",
        "memory._add_to_vector_store = memory_patched_add.__get__(memory, memory.__class__)\n",
        "\n",
        "semantic_memory._search_vector_store = patched_search_vector_store.__get__(semantic_memory, semantic_memory.__class__)\n",
        "semantic_memory._add_to_vector_store = semantic_memory_patched_add.__get__(semantic_memory, semantic_memory.__class__)\n",
        "\n",
        "print(\"ğŸ”§ MongoDB search and add methods patched successfully for both memory instances!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0dd50a39",
      "metadata": {},
      "source": [
        "## Data Ingestion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7b20bfac",
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "# æ–¹æ³• 1ï¼šå°†é¡¹ç›®æ ¹ç›®å½•æ·»åŠ åˆ° Python è·¯å¾„\n",
        "project_root = \"/Users/binzhou/Demo/agent_memory_demo/part1\"\n",
        "if project_root not in sys.path:\n",
        "    sys.path.insert(0, project_root)\n",
        "\n",
        "from utilities.pdf_chunker import ingest_pdf_and_chunk\n",
        "\n",
        "url = \"https://arxiv.org/pdf/2404.13501\"\n",
        "\n",
        "# æ‘„å–å¹¶åˆ†å— PDF\n",
        "chunks = ingest_pdf_and_chunk(url)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "260e2af4",
      "metadata": {},
      "outputs": [],
      "source": [
        "# å°†åˆ†å—é€ä¸ªæ‘„å…¥è¯­ä¹‰è®°å¿†ï¼Œé¿å…è¶…æ—¶\n",
        "\n",
        "print(f\"ğŸ“„ Ingesting {len(chunks)} chunks into semantic memory...\")\n",
        "\n",
        "successful_ingestions = 0\n",
        "failed_ingestions = 0\n",
        "\n",
        "for i, chunk in enumerate(chunks):\n",
        "    try:\n",
        "        # é€ä¸ªå¤„ç†æ¯ä¸ªåˆ†å—\n",
        "        chunk_content = (chunk.get(\"value\") or {}).get(\"content\") or \"\"\n",
        "        if not chunk_content or len(chunk_content.strip()) < 50:\n",
        "            continue\n",
        "            \n",
        "        # ä¸ºè¯¥åˆ†å—åˆ›å»ºå•æ¡æ¶ˆæ¯\n",
        "        message = {\"role\": \"user\", \"content\": chunk_content}\n",
        "        \n",
        "        # æ·»åŠ åˆ°è¯­ä¹‰è®°å¿†\n",
        "        result = semantic_memory.add([message], user_id=CURRENT_USER_ID, infer=False)\n",
        "        successful_ingestions += 1\n",
        "        \n",
        "        if (i + 1) % 10 == 0:  # æ¯å¤„ç† 10 ä¸ªåˆ†å—æ›´æ–°è¿›åº¦\n",
        "            print(f\"   Processed {i + 1}/{len(chunks)} chunks...\")\n",
        "            \n",
        "    except Exception as e:\n",
        "        print(f\"   âŒ Failed to ingest chunk {i}: {str(e)[:100]}...\")\n",
        "        failed_ingestions += 1\n",
        "        continue\n",
        "\n",
        "print(f\"âœ… Ingestion complete!\")\n",
        "print(f\"   - Successfully ingested: {successful_ingestions} chunks\")\n",
        "print(f\"   - Failed: {failed_ingestions} chunks\")\n",
        "print(f\"   - Total processed: {successful_ingestions + failed_ingestions}/{len(chunks)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cbba1d97",
      "metadata": {},
      "outputs": [],
      "source": [
        "def chat_with_memories(message: str, user_id: str = None) -> str:\n",
        "    \"\"\"\n",
        "    Chat function that retrieves relevant memories and generates responses.\n",
        "    \n",
        "    Args:\n",
        "        message (str): User's input message\n",
        "        user_id (str): User identifier for memory retrieval (default: CURRENT_USER_ID)\n",
        "        \n",
        "    Returns:\n",
        "        str: Assistant's response based on memories and query\n",
        "    \"\"\"\n",
        "    if user_id is None:\n",
        "        user_id = CURRENT_USER_ID\n",
        "    if not message or not message.strip():\n",
        "        return \"Please enter a message.\"\n",
        "    try:\n",
        "        # Retrieve relevant memories using the fixed search method\n",
        "        relevant_memories = memory.search(query=message, user_id=user_id, limit=3)\n",
        "        relevant_memories_from_semantic_memory = semantic_memory.search(query=message, user_id=user_id, limit=3)\n",
        "        \n",
        "        # Format memories for display and context (defensive: .get(\"results\", []) to avoid KeyError)\n",
        "        memories_str = \"\\n\".join(f\"- {entry['memory']}\" for entry in relevant_memories.get(\"results\", []))\n",
        "        memories_str_from_semantic_memory = \"\\n\".join(f\"- {entry['memory']}\" for entry in relevant_memories_from_semantic_memory.get(\"results\", []))\n",
        "        print(\"Memories:\")\n",
        "        print(memories_str if memories_str else \"No relevant memories found.\")\n",
        "        # print(\"Semantic Memories:\")\n",
        "        # print(memories_str_from_semantic_memory if memories_str_from_semantic_memory else \"No relevant semantic memories found.\")\n",
        "\n",
        "        # Generate Assistant response with memory context\n",
        "        system_prompt = f\"You are a helpful AI. Answer the question based on the query and memories.\\nUser Memories:\\n{memories_str}\\nSemantic Memories:\\n{memories_str_from_semantic_memory}\"\n",
        "        messages = [{\"role\": \"system\", \"content\": system_prompt}, {\"role\": \"user\", \"content\": message}]\n",
        "        \n",
        "        response = openai_client.chat.completions.create(model=\"gpt-4o\", messages=messages)\n",
        "        assistant_response = response.choices[0].message.content\n",
        "\n",
        "        # Create new memories from the conversation (same user_id ä¿è¯ä¸‹æ¬¡èƒ½æ£€ç´¢åˆ°)\n",
        "        messages.append({\"role\": \"assistant\", \"content\": assistant_response})\n",
        "        try:\n",
        "            memory.add(messages, user_id=user_id)\n",
        "        except Exception as add_err:\n",
        "            print(f\"Warning: è®°å¿†å†™å…¥å¤±è´¥ï¼Œä¸‹æ¬¡å¯èƒ½æ— æ³•è®°ä½æœ¬æ¬¡å¯¹è¯: {add_err}\")\n",
        "\n",
        "        return assistant_response\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"Error in chat_with_memories: {e}\")\n",
        "        # Fallback response without memory\n",
        "        messages = [{\"role\": \"user\", \"content\": message}]\n",
        "        response = openai_client.chat.completions.create(model=\"gpt-4o\", messages=messages)\n",
        "        return response.choices[0].message.content\n",
        "\n",
        "def main():\n",
        "    \"\"\"Interactive chat loop (for terminal; in Jupyter use chat_with_memories('your question') in a cell)\"\"\"\n",
        "    print(\"Chat with AI (type 'exit' to quit)\")\n",
        "    print(\"This AI has memory and can remember your conversation!\")\n",
        "    try:\n",
        "        while True:\n",
        "            user_input = input(\"\\nYou: \").strip()\n",
        "            if user_input.lower() == 'exit':\n",
        "                print(\"Goodbye!\")\n",
        "                break\n",
        "            try:\n",
        "                response = chat_with_memories(user_input)\n",
        "                print(f\"AI: {response}\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error: {e}\")\n",
        "    except Exception as e:\n",
        "        if \"StdinNotImplementedError\" in type(e).__name__ or \"input\" in str(e).lower():\n",
        "            print(\"Notebook: run chat_with_memories('your question') in a cell instead.\")\n",
        "        else:\n",
        "            raise\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c521daa6",
      "metadata": {},
      "outputs": [],
      "source": [
        "coffee_search = memory.search(query=\"coffee brewing methods and preferences\", user_id=CURRENT_USER_ID, limit=5)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e79a22cd",
      "metadata": {},
      "outputs": [],
      "source": [
        "coffee_search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "49ebb327",
      "metadata": {},
      "outputs": [],
      "source": [
        "paper_search = semantic_memory.search(query=\"What is the main idea of the paper?\", user_id=CURRENT_USER_ID, limit=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c400608e",
      "metadata": {},
      "outputs": [],
      "source": [
        "paper_search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "36152685",
      "metadata": {},
      "outputs": [],
      "source": [
        "chat_with_memories(\"æˆ‘å«ä»€ä¹ˆåå­—ï¼Ÿæˆ‘å–œæ¬¢åƒä»€ä¹ˆæ°´æœï¼Ÿ\",user_id=CURRENT_USER_ID)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv (3.12.12)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
