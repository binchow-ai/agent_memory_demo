{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "04323d1d",
      "metadata": {},
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github.com/RichmondAlake/agent_memory_course/blob/main/mem0/memory_augmented_agent_with_mem0_mongodb.ipynb)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "49e1ccaa",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install -qU mem0ai azure-identity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "9b701237",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Â∏∏Èáè\n",
        "CURRENT_USER_ID = \"user-123\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "9bab6374",
      "metadata": {},
      "outputs": [],
      "source": [
        "import getpass\n",
        "import os\n",
        "\n",
        "try:\n",
        "    from dotenv import load_dotenv\n",
        "    load_dotenv()\n",
        "except ImportError:\n",
        "    pass  # Êó† python-dotenv Êó∂‰ΩøÁî®Á≥ªÁªüÁéØÂ¢ÉÂèòÈáè\n",
        "\n",
        "# ÂÆâÂÖ®Âú∞Ëé∑ÂèñÂπ∂ËÆæÁΩÆÁéØÂ¢ÉÂèòÈáèÁöÑÂáΩÊï∞\n",
        "def set_env_securely(var_name, prompt):\n",
        "    value = getpass.getpass(prompt)\n",
        "    os.environ[var_name] = value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "207f4977",
      "metadata": {},
      "outputs": [],
      "source": [
        "# set_env_securely(\"OPENAI_API_KEY\", \"Enter your OPENAI_API_KEY: \")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "d5a1efef",
      "metadata": {},
      "outputs": [],
      "source": [
        "# set_env_securely(\"MONGODB_URI\", \"Enter your MongoDB URI: \")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "c8a7f177",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/Users/binzhou/Demo/agent_memory_demo/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "# config = {\n",
        "#     \"vector_store\": {\n",
        "#         \"provider\": \"mongodb\",\n",
        "#         \"config\": {\n",
        "#             \"db_name\": \"mem0_agent_memory\",\n",
        "#             \"collection_name\": \"extracted_memories\",\n",
        "#             \"mongo_uri\": os.environ[\"MONGODB_URI\"]\n",
        "#         }\n",
        "#     }\n",
        "# }\n",
        "from langchain_voyageai import VoyageAIEmbeddings\n",
        "\n",
        "config = {\n",
        "    \"vector_store\": {\n",
        "        \"provider\": \"mongodb\",\n",
        "        \"config\": {\n",
        "            \"db_name\": \"mem0_agent_memory\",\n",
        "            \"collection_name\": \"extracted_memories\",\n",
        "            \"mongo_uri\": os.environ[\"MONGODB_URI\"],\n",
        "            \"embedding_model_dims\": 1024\n",
        "        }\n",
        "    },\n",
        "    \"embedder\": {\n",
        "        \"provider\": \"langchain\",\n",
        "        \"config\": {\n",
        "            \"model\": VoyageAIEmbeddings(\n",
        "                model=\"voyage-3-large\",\n",
        "                voyage_api_key=os.getenv(\"VOYAGE_API_KEY\")\n",
        "            ),\n",
        "            \"embedding_dims\": 1024\n",
        "        }\n",
        "    },\n",
        "    \"llm\": {\n",
        "        \"provider\": \"azure_openai\",\n",
        "        \"config\": {\n",
        "            \"model\": \"gpt-4o\",\n",
        "            \"azure_kwargs\": {\n",
        "                \"api_key\": os.getenv(\"OPENAI_API_KEY\"),\n",
        "                \"azure_endpoint\": os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
        "                \"azure_deployment\": \"gpt-4o\",\n",
        "                \"api_version\": \"2025-03-01-preview\"\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "# semantic_memory_config = {\n",
        "#     \"vector_store\": {\n",
        "#         \"provider\": \"mongodb\",\n",
        "#         \"config\": {\n",
        "#             \"db_name\": \"mem0_agent_memory\",\n",
        "#             \"collection_name\": \"semantic_memory\",\n",
        "#             \"mongo_uri\": os.environ[\"MONGODB_URI\"]\n",
        "#         }\n",
        "#     }\n",
        "# }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "02f078ef",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Âú® semantic_memory_configÔºà‰ª•ÂèäÈúÄË¶ÅÁî®Âà∞ Voyage ÁöÑ configÔºâÈáåÔºö\n",
        "semantic_memory_config = {\n",
        "    \"vector_store\": {\n",
        "        \"provider\": \"mongodb\",\n",
        "        \"config\": {\n",
        "            \"db_name\": \"mem0_agent_memory\",\n",
        "            \"collection_name\": \"semantic_memory\",\n",
        "            \"mongo_uri\": os.environ[\"MONGODB_URI\"],\n",
        "            \"embedding_model_dims\": 1024\n",
        "        }\n",
        "    },\n",
        "    \"embedder\": {\n",
        "        \"provider\": \"langchain\",\n",
        "        \"config\": {\n",
        "            \"model\": VoyageAIEmbeddings(\n",
        "                model=\"voyage-3-large\",\n",
        "                voyage_api_key=os.getenv(\"VOYAGE_API_KEY\")\n",
        "            ),\n",
        "            \"embedding_dims\": 1024\n",
        "        }\n",
        "    }\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "6230bb8c",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dropped search index 'extracted_memories_vector_index'.\n",
            "Drop 'semantic_memory_vector_index': Search index mem0_agent_memory.semantic_memory.semantic_memory_vector_index cannot be found, full error: {'ok': 0.0, 'errmsg': 'Search index mem0_agent_memory.semantic_memory.semantic_memory_vector_index cannot be found', 'code': 27, 'codeName': 'IndexNotFound', '$clusterTime': {'clusterTime': Timestamp(1770115086, 1), 'signature': {'hash': b'i\\x8c\\xa7\\xdco\\x99\\x8e\\x11N-\\xad\\x90 6\\x906p\\x97hO', 'keyId': 7584727213249921030}}, 'operationTime': Timestamp(1770115086, 1)}\n"
          ]
        }
      ],
      "source": [
        "# Ëã•ÊõæÁî® 1536 Áª¥ÔºàÂ¶Ç OpenAIÔºâÂª∫ËøáÁ¥¢ÂºïÔºåÈúÄÂÖàÂà†ÊéâÊóßÁ¥¢ÂºïÔºåÂÜçË∑ë‰∏ãÈù¢ Memory.from_config ‰ºöÊåâ 1024 Áª¥ÔºàVoyageÔºâÈáçÂª∫„ÄÇ\n",
        "# Âè™ÈúÄÂú®Âá∫Áé∞ \"vector field is indexed with 1536 dimensions but queried with 1024\" Êó∂ËøêË°å‰∏ÄÊ¨°Âç≥ÂèØ„ÄÇ\n",
        "from pymongo import MongoClient\n",
        "_client = MongoClient(os.environ[\"MONGODB_URI\"])\n",
        "_db = _client[\"mem0_agent_memory\"]\n",
        "for _coll_name in [\"extracted_memories\", \"semantic_memory\"]:\n",
        "    _idx_name = f\"{_coll_name}_vector_index\"\n",
        "    try:\n",
        "        _db[_coll_name].drop_search_index(_idx_name)\n",
        "        print(f\"Dropped search index '{_idx_name}'.\")\n",
        "    except Exception as e:\n",
        "        if \"index not found\" in str(e).lower() or \"no such index\" in str(e).lower():\n",
        "            print(f\"Index '{_idx_name}' not found (ok).\")\n",
        "        else:\n",
        "            print(f\"Drop '{_idx_name}': {e}\")\n",
        "_client.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "03665250",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Created index 'extracted_memories_vector_index' (may take a minute on Atlas).\n",
            "Index 'semantic_memory_vector_index' already exists.\n"
          ]
        }
      ],
      "source": [
        "# Ëã•Êä•Èîô \"Index 'extracted_memories_vector_index' does not exist\"ÔºåÂÖàËøêË°åÊú¨ cell ÂàõÂª∫Á¥¢ÂºïÔºåÂÜçÈáçÊñ∞ËøêË°å‰∏ãÈù¢ÁöÑ Memory.from_config\n",
        "from pymongo import MongoClient\n",
        "from pymongo.operations import SearchIndexModel\n",
        "\n",
        "_uri = os.environ.get(\"MONGODB_URI\")\n",
        "if _uri:\n",
        "    _c = MongoClient(_uri)\n",
        "    _db = _c[\"mem0_agent_memory\"]\n",
        "    _dims = 1024\n",
        "    for _coll_name in [\"extracted_memories\", \"semantic_memory\"]:\n",
        "        _col = _db[_coll_name]\n",
        "        _idx_name = f\"{_coll_name}_vector_index\"\n",
        "        _existing = list(_col.list_search_indexes(name=_idx_name))\n",
        "        if _existing:\n",
        "            print(f\"Index '{_idx_name}' already exists.\")\n",
        "        else:\n",
        "            _col.create_search_index(SearchIndexModel(\n",
        "                name=_idx_name,\n",
        "                definition={\n",
        "                    \"mappings\": {\n",
        "                        \"dynamic\": False,\n",
        "                        \"fields\": {\n",
        "                            \"embedding\": {\"type\": \"knnVector\", \"dimensions\": _dims, \"similarity\": \"cosine\"}\n",
        "                        }\n",
        "                    }\n",
        "                }\n",
        "            ))\n",
        "            print(f\"Created index '{_idx_name}' (may take a minute on Atlas).\")\n",
        "    _c.close()\n",
        "else:\n",
        "    print(\"MONGODB_URI not set.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "805e0f8e",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:mem0.vector_stores.mongodb:Search index 'extracted_memories_vector_index' already exists in collection 'extracted_memories'.\n",
            "INFO:mem0.vector_stores.mongodb:Search index 'mem0migrations_vector_index' already exists in collection 'mem0migrations'.\n",
            "INFO:mem0.vector_stores.mongodb:Retrieved document with ID 'c0c3c749-98eb-4d92-9620-8283735e0a66'.\n",
            "INFO:mem0.vector_stores.mongodb:Search index 'semantic_memory_vector_index' already exists in collection 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Search index 'mem0migrations_vector_index' already exists in collection 'mem0migrations'.\n",
            "INFO:mem0.vector_stores.mongodb:Retrieved document with ID 'c0c3c749-98eb-4d92-9620-8283735e0a66'.\n"
          ]
        }
      ],
      "source": [
        "from mem0 import Memory\n",
        "memory = Memory.from_config(config)\n",
        "semantic_memory = Memory.from_config(semantic_memory_config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "7dce7b40",
      "metadata": {},
      "outputs": [],
      "source": [
        "from openai import OpenAI\n",
        "from typing import Optional, Dict\n",
        "\n",
        "# ‰ΩøÁî® Azure Êó∂Âä†‰∏äËøôË°åÔºåÂê¶Âàô‰ºöËµ∞Âà∞ Azure ÁöÑ tracing Á≥ªÁªüÔºåÊä•401Error\n",
        "os.environ[\"OPENAI_AGENTS_DISABLE_TRACING\"] = \"true\"\n",
        "\n",
        "# ÂØºÂÖ• OpenAI Python ÂÆ¢Êà∑Á´ØÂ∫ì\n",
        "# from openai import OpenAI\n",
        "from openai import AzureOpenAI\n",
        "\n",
        "# ÂàùÂßãÂåñ OpenAI ÂÆ¢Êà∑Á´Ø\n",
        "# Â∞Ü‰ΩøÁî®ÁéØÂ¢ÉÂèòÈáè OPENAI_API_KEY ‰∏≠ÁöÑ API ÂØÜÈí•\n",
        "#openai_client = OpenAI()\n",
        "openai_client = AzureOpenAI(\n",
        "    #api_version=\"2024-12-01-preview\",\n",
        "    api_version=\"2025-03-01-preview\",\n",
        "    azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
        "    api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
        "    azure_deployment=\"gpt-4o\",\n",
        ")\n",
        "\n",
        "\n",
        "def patched_search_vector_store(self, query, filters, limit, threshold: Optional[float] = None):\n",
        "    \"\"\"\n",
        "    ‰ΩøÁî®Ê≠£Á°Æ MongoDB ÂèÇÊï∞ÂêçÁöÑ _search_vector_store Ë°•‰∏ÅÁâàÊú¨„ÄÇ\n",
        "    \n",
        "    mem0 MongoDB search Á≠æÂêç‰∏∫ (query, vectors, limit, filters)ÔºåÂèÇÊï∞Âêç‰∏∫ vectors„ÄÇ\n",
        "    \"\"\"\n",
        "    if not query or (isinstance(query, str) and not query.strip()):\n",
        "        return []\n",
        "    # ‰∏∫Êü•ËØ¢ÁîüÊàêÂµåÂÖ•ÂêëÈáè\n",
        "    embeddings = self.embedding_model.embed(query, \"search\")\n",
        "    memories = self.vector_store.search(query=query, vectors=embeddings, limit=limit, filters=filters)\n",
        "    \n",
        "    # Â∞ÜÁªìÊûúÂ§ÑÁêÜ‰∏∫ÊúüÊúõÁöÑÊ†ºÂºè\n",
        "    promoted_payload_keys = [\n",
        "        \"user_id\",\n",
        "        \"agent_id\", \n",
        "        \"run_id\",\n",
        "        \"actor_id\",\n",
        "        \"role\",\n",
        "    ]\n",
        "\n",
        "    core_and_promoted_keys = {\"data\", \"hash\", \"created_at\", \"updated_at\", \"id\", *promoted_payload_keys}\n",
        "\n",
        "    # Ëã•ÊåáÂÆöÂàôÂ∫îÁî®ÈòàÂÄºËøáÊª§\n",
        "    if threshold is not None:\n",
        "        filtered_memories = []\n",
        "        for memory_item in memories:\n",
        "            if hasattr(memory_item, 'score') and memory_item.score >= threshold:\n",
        "                filtered_memories.append(memory_item)\n",
        "        memories = filtered_memories\n",
        "\n",
        "    # Â∞ÜËÆ∞ÂøÜÂ∫èÂàóÂåñ‰∏∫ÊúüÊúõÁöÑÊ†ºÂºè\n",
        "    serialized_memories = []\n",
        "    for memory_item in memories:\n",
        "        serialized_memory = {}\n",
        "        \n",
        "        # Â§ÑÁêÜÊ†∏ÂøÉÂ≠óÊÆµ\n",
        "        for key in core_and_promoted_keys:\n",
        "            if key == \"data\":\n",
        "                # ÂÆûÈôÖËÆ∞ÂøÜÂÜÖÂÆπÂú® payload ‰∏≠\n",
        "                serialized_memory[key] = getattr(memory_item, 'payload', {}).get('data', None)\n",
        "            else:\n",
        "                serialized_memory[key] = getattr(memory_item, key, None)\n",
        "\n",
        "        # Ê∑ªÂä†ÂàÜÊï∞ÂíåËÆ∞ÂøÜÂÜÖÂÆπ\n",
        "        serialized_memory[\"score\"] = getattr(memory_item, \"score\", None)\n",
        "        serialized_memory[\"memory\"] = getattr(memory_item, 'payload', {}).get('data', '')\n",
        "        serialized_memory[\"metadata\"] = getattr(memory_item, 'payload', {}).get('metadata', {})\n",
        "\n",
        "        # Ê∑ªÂä†ÊèêÂçáÁöÑ payload ÈîÆ\n",
        "        payload = getattr(memory_item, 'payload', {})\n",
        "        for key in promoted_payload_keys:\n",
        "            value = payload.get(key, None)\n",
        "            if value is not None:\n",
        "                serialized_memory[key] = value\n",
        "\n",
        "        serialized_memories.append(serialized_memory)\n",
        "\n",
        "    return serialized_memories\n",
        "\n",
        "def create_patched_add_to_vector_store(original_method):\n",
        "    \"\"\"\n",
        "    Â∑•ÂéÇÂáΩÊï∞ÔºåÁî®‰∫éÂàõÂª∫Ë°•‰∏ÅÁâàÁöÑ _add_to_vector_store ÊñπÊ≥ï„ÄÇ\n",
        "    Á°Æ‰øùÊØè‰∏™ËÆ∞ÂøÜÂÆû‰æãÈÉΩÊúâËá™Â∑±ÂØπÂéüÂßãÊñπÊ≥ïÁöÑÂºïÁî®„ÄÇ\n",
        "    \"\"\"\n",
        "    def patched_add_to_vector_store(self, messages, metadata, filters, infer):\n",
        "        \"\"\"\n",
        "        ‰øÆÂ§ç MongoDB ÊêúÁ¥¢ÂèÇÊï∞ÈóÆÈ¢òÁöÑ _add_to_vector_store Ë°•‰∏ÅÁâàÊú¨„ÄÇ\n",
        "        \n",
        "        ËØ•ÊñπÊ≥ïÂú® memory.add() Êìç‰ΩúÊúüÈó¥Ë¢´Ë∞ÉÁî®ÔºåÊ≠§Ââç‰πüÈîôËØØÂú∞‰ΩøÁî®‰∫Ü 'vectors' ÂèÇÊï∞„ÄÇ\n",
        "        \"\"\"\n",
        "        # ‰ªéÂéüÂßãÊ®°ÂùóÂØºÂÖ• logger\n",
        "        import logging\n",
        "        logger = logging.getLogger(__name__)\n",
        "        \n",
        "        # Ëé∑ÂèñÂÆûÈôÖÁöÑ MongoDB ÂêëÈáèÂ≠òÂÇ®ÊêúÁ¥¢ÊñπÊ≥ïÔºàÈùûÊàë‰ª¨ÁöÑË°•‰∏ÅÁâàÊú¨Ôºâ\n",
        "        mongodb_vector_store = self.vector_store\n",
        "        \n",
        "        # Áõ¥Êé•Ëé∑ÂèñÂéüÂßã MongoDB ÊêúÁ¥¢ÊñπÊ≥ï\n",
        "        original_mongodb_search = mongodb_vector_store.__class__.search\n",
        "        \n",
        "        def patched_vector_store_search(self_vs, query, vectors, limit, filters):\n",
        "            return original_mongodb_search(self_vs, query=query, vectors=vectors, limit=limit, filters=filters)\n",
        "        \n",
        "        # ‰∏¥Êó∂Ë°•‰∏ÅÂêëÈáèÂ≠òÂÇ®ÁöÑÊêúÁ¥¢ÊñπÊ≥ï\n",
        "        original_vector_store_search = mongodb_vector_store.search\n",
        "        mongodb_vector_store.search = patched_vector_store_search.__get__(mongodb_vector_store, mongodb_vector_store.__class__)\n",
        "        \n",
        "        try:\n",
        "            # ‰ª•ÁªëÂÆöÊñπÊ≥ïÂΩ¢ÂºèË∞ÉÁî®ÂéüÂßã _add_to_vector_store ÊñπÊ≥ï\n",
        "            result = original_method.__get__(self, type(self))(messages, metadata, filters, infer)\n",
        "            return result\n",
        "        finally:\n",
        "            # ÊÅ¢Â§çÂéüÂßãÂêëÈáèÂ≠òÂÇ®ÁöÑÊêúÁ¥¢ÊñπÊ≥ï\n",
        "            mongodb_vector_store.search = original_vector_store_search\n",
        "    \n",
        "    return patched_add_to_vector_store"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "1a4d047f",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîß MongoDB search and add methods patched successfully for both memory instances!\n"
          ]
        }
      ],
      "source": [
        "# Âú®Á±ªÂíåÂÆû‰æã‰∏§‰∏™Â±ÇÁ∫ßÂ∫îÁî®Áå¥Â≠êË°•‰∏Å\n",
        "from mem0.memory.main import Memory\n",
        "\n",
        "# Âú®ÊâìË°•‰∏Å‰πãÂâç‰øùÂ≠òÊØè‰∏™ÂÆû‰æãÁöÑÂéüÂßãÊñπÊ≥ï\n",
        "memory_original_search = memory._search_vector_store\n",
        "memory_original_add = memory._add_to_vector_store\n",
        "semantic_memory_original_search = semantic_memory._search_vector_store\n",
        "semantic_memory_original_add = semantic_memory._add_to_vector_store\n",
        "\n",
        "# Âú®‰ªª‰ΩïË°•‰∏Å‰πãÂâç‰øùÂ≠òÂéüÂßãÁ±ªÊñπÊ≥ï\n",
        "original_class_search = Memory._search_vector_store\n",
        "original_class_add = Memory._add_to_vector_store\n",
        "\n",
        "# Â∫îÁî®Á±ªÁ∫ßÂà´Ë°•‰∏ÅÔºàÂΩ±ÂìçÊñ∞ÂÆû‰æãÔºâ\n",
        "Memory._search_vector_store = patched_search_vector_store\n",
        "Memory._add_to_vector_store = create_patched_add_to_vector_store(original_class_add)\n",
        "\n",
        "# ‰∏∫ÂêÑÂÆû‰æãÂàõÂª∫‰∏ìÁî®ÁöÑË°•‰∏Å add ÊñπÊ≥ï\n",
        "memory_patched_add = create_patched_add_to_vector_store(memory_original_add)\n",
        "semantic_memory_patched_add = create_patched_add_to_vector_store(semantic_memory_original_add)\n",
        "\n",
        "# ÂØπÂ∑≤ÊúâÂÆû‰æãÂ∫îÁî®Ë°•‰∏Å\n",
        "memory._search_vector_store = patched_search_vector_store.__get__(memory, memory.__class__)\n",
        "memory._add_to_vector_store = memory_patched_add.__get__(memory, memory.__class__)\n",
        "\n",
        "semantic_memory._search_vector_store = patched_search_vector_store.__get__(semantic_memory, semantic_memory.__class__)\n",
        "semantic_memory._add_to_vector_store = semantic_memory_patched_add.__get__(semantic_memory, semantic_memory.__class__)\n",
        "\n",
        "print(\"üîß MongoDB search and add methods patched successfully for both memory instances!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0dd50a39",
      "metadata": {},
      "source": [
        "## Data Ingestion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "7b20bfac",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading PDF from https://arxiv.org/pdf/2404.13501...\n",
            "Loading PDF with LangChain...\n",
            "Chunking text...\n",
            "Successfully created 198 chunks\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "\n",
        "# ÊñπÊ≥ï 1ÔºöÂ∞ÜÈ°πÁõÆÊ†πÁõÆÂΩïÊ∑ªÂä†Âà∞ Python Ë∑ØÂæÑ\n",
        "project_root = \"/Users/binzhou/Demo/agent_memory_demo/part1\"\n",
        "if project_root not in sys.path:\n",
        "    sys.path.insert(0, project_root)\n",
        "\n",
        "from utilities.pdf_chunker import ingest_pdf_and_chunk\n",
        "\n",
        "url = \"https://arxiv.org/pdf/2404.13501\"\n",
        "\n",
        "# ÊëÑÂèñÂπ∂ÂàÜÂùó PDF\n",
        "chunks = ingest_pdf_and_chunk(url)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "260e2af4",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìÑ Ingesting 198 chunks into semantic memory...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: HEAD https://huggingface.co/voyageai/voyage-3-large/resolve/main/tokenizer.json \"HTTP/1.1 307 Temporary Redirect\"\n",
            "INFO:httpx:HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/voyageai/voyage-3-large/d856baa31d0d467511b9f55f681bd2b17350a46c/tokenizer.json \"HTTP/1.1 200 OK\"\n",
            "INFO:mem0.vector_stores.mongodb:Inserting 1 vectors into collection 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserted 1 documents into 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserting 1 vectors into collection 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserted 1 documents into 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserting 1 vectors into collection 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserted 1 documents into 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserting 1 vectors into collection 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserted 1 documents into 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserting 1 vectors into collection 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserted 1 documents into 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserting 1 vectors into collection 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserted 1 documents into 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserting 1 vectors into collection 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserted 1 documents into 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserting 1 vectors into collection 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserted 1 documents into 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserting 1 vectors into collection 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserted 1 documents into 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserting 1 vectors into collection 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserted 1 documents into 'semantic_memory'.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   Processed 10/198 chunks...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:mem0.vector_stores.mongodb:Inserting 1 vectors into collection 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserted 1 documents into 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserting 1 vectors into collection 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserted 1 documents into 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserting 1 vectors into collection 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserted 1 documents into 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserting 1 vectors into collection 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserted 1 documents into 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserting 1 vectors into collection 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserted 1 documents into 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserting 1 vectors into collection 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserted 1 documents into 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserting 1 vectors into collection 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserted 1 documents into 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserting 1 vectors into collection 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserted 1 documents into 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserting 1 vectors into collection 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserted 1 documents into 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserting 1 vectors into collection 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserted 1 documents into 'semantic_memory'.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   Processed 20/198 chunks...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:mem0.vector_stores.mongodb:Inserting 1 vectors into collection 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserted 1 documents into 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserting 1 vectors into collection 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserted 1 documents into 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserting 1 vectors into collection 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserted 1 documents into 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserting 1 vectors into collection 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserted 1 documents into 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserting 1 vectors into collection 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserted 1 documents into 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserting 1 vectors into collection 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserted 1 documents into 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserting 1 vectors into collection 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserted 1 documents into 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserting 1 vectors into collection 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserted 1 documents into 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserting 1 vectors into collection 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserted 1 documents into 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserting 1 vectors into collection 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserted 1 documents into 'semantic_memory'.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   Processed 30/198 chunks...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:mem0.vector_stores.mongodb:Inserting 1 vectors into collection 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserted 1 documents into 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserting 1 vectors into collection 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserted 1 documents into 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserting 1 vectors into collection 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserted 1 documents into 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserting 1 vectors into collection 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserted 1 documents into 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserting 1 vectors into collection 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserted 1 documents into 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserting 1 vectors into collection 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserted 1 documents into 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserting 1 vectors into collection 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserted 1 documents into 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserting 1 vectors into collection 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserted 1 documents into 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserting 1 vectors into collection 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserted 1 documents into 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserting 1 vectors into collection 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserted 1 documents into 'semantic_memory'.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   Processed 40/198 chunks...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:mem0.vector_stores.mongodb:Inserting 1 vectors into collection 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserted 1 documents into 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserting 1 vectors into collection 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserted 1 documents into 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserting 1 vectors into collection 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserted 1 documents into 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserting 1 vectors into collection 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserted 1 documents into 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserting 1 vectors into collection 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserted 1 documents into 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserting 1 vectors into collection 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserted 1 documents into 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserting 1 vectors into collection 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserted 1 documents into 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserting 1 vectors into collection 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserted 1 documents into 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserting 1 vectors into collection 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserted 1 documents into 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserting 1 vectors into collection 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserted 1 documents into 'semantic_memory'.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   Processed 50/198 chunks...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:mem0.vector_stores.mongodb:Inserting 1 vectors into collection 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserted 1 documents into 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserting 1 vectors into collection 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserted 1 documents into 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserting 1 vectors into collection 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserted 1 documents into 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserting 1 vectors into collection 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserted 1 documents into 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserting 1 vectors into collection 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserted 1 documents into 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserting 1 vectors into collection 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserted 1 documents into 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserting 1 vectors into collection 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserted 1 documents into 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserting 1 vectors into collection 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserted 1 documents into 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserting 1 vectors into collection 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserted 1 documents into 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserting 1 vectors into collection 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserted 1 documents into 'semantic_memory'.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   Processed 60/198 chunks...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:mem0.vector_stores.mongodb:Inserting 1 vectors into collection 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserted 1 documents into 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserting 1 vectors into collection 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserted 1 documents into 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserting 1 vectors into collection 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserted 1 documents into 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserting 1 vectors into collection 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserted 1 documents into 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserting 1 vectors into collection 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserted 1 documents into 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserting 1 vectors into collection 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserted 1 documents into 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserting 1 vectors into collection 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserted 1 documents into 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserting 1 vectors into collection 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserted 1 documents into 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserting 1 vectors into collection 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserted 1 documents into 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserting 1 vectors into collection 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserted 1 documents into 'semantic_memory'.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   Processed 70/198 chunks...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:mem0.vector_stores.mongodb:Inserting 1 vectors into collection 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserted 1 documents into 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserting 1 vectors into collection 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserted 1 documents into 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserting 1 vectors into collection 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserted 1 documents into 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserting 1 vectors into collection 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserted 1 documents into 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserting 1 vectors into collection 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserted 1 documents into 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserting 1 vectors into collection 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserted 1 documents into 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserting 1 vectors into collection 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserted 1 documents into 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserting 1 vectors into collection 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserted 1 documents into 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserting 1 vectors into collection 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserted 1 documents into 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserting 1 vectors into collection 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserted 1 documents into 'semantic_memory'.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   Processed 80/198 chunks...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:mem0.vector_stores.mongodb:Inserting 1 vectors into collection 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserted 1 documents into 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserting 1 vectors into collection 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserted 1 documents into 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserting 1 vectors into collection 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserted 1 documents into 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserting 1 vectors into collection 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserted 1 documents into 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserting 1 vectors into collection 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserted 1 documents into 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserting 1 vectors into collection 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserted 1 documents into 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserting 1 vectors into collection 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserted 1 documents into 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserting 1 vectors into collection 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserted 1 documents into 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserting 1 vectors into collection 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserted 1 documents into 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserting 1 vectors into collection 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserted 1 documents into 'semantic_memory'.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   Processed 90/198 chunks...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:mem0.vector_stores.mongodb:Inserting 1 vectors into collection 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserted 1 documents into 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserting 1 vectors into collection 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserted 1 documents into 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserting 1 vectors into collection 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserted 1 documents into 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserting 1 vectors into collection 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserted 1 documents into 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserting 1 vectors into collection 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserted 1 documents into 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserting 1 vectors into collection 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserted 1 documents into 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserting 1 vectors into collection 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserted 1 documents into 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserting 1 vectors into collection 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserted 1 documents into 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserting 1 vectors into collection 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserted 1 documents into 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserting 1 vectors into collection 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserted 1 documents into 'semantic_memory'.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   Processed 100/198 chunks...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:mem0.vector_stores.mongodb:Inserting 1 vectors into collection 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserted 1 documents into 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserting 1 vectors into collection 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserted 1 documents into 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserting 1 vectors into collection 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserted 1 documents into 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserting 1 vectors into collection 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserted 1 documents into 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserting 1 vectors into collection 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserted 1 documents into 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserting 1 vectors into collection 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserted 1 documents into 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserting 1 vectors into collection 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserted 1 documents into 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserting 1 vectors into collection 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserted 1 documents into 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserting 1 vectors into collection 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserted 1 documents into 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserting 1 vectors into collection 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserted 1 documents into 'semantic_memory'.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   Processed 110/198 chunks...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:mem0.vector_stores.mongodb:Inserting 1 vectors into collection 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserted 1 documents into 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserting 1 vectors into collection 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserted 1 documents into 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserting 1 vectors into collection 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserted 1 documents into 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserting 1 vectors into collection 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserted 1 documents into 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserting 1 vectors into collection 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserted 1 documents into 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserting 1 vectors into collection 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserted 1 documents into 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserting 1 vectors into collection 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserted 1 documents into 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserting 1 vectors into collection 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserted 1 documents into 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserting 1 vectors into collection 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserted 1 documents into 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserting 1 vectors into collection 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserted 1 documents into 'semantic_memory'.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   Processed 120/198 chunks...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:mem0.vector_stores.mongodb:Inserting 1 vectors into collection 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserted 1 documents into 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserting 1 vectors into collection 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserted 1 documents into 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserting 1 vectors into collection 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserted 1 documents into 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserting 1 vectors into collection 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserted 1 documents into 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserting 1 vectors into collection 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserted 1 documents into 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserting 1 vectors into collection 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserted 1 documents into 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserting 1 vectors into collection 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserted 1 documents into 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserting 1 vectors into collection 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserted 1 documents into 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserting 1 vectors into collection 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserted 1 documents into 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserting 1 vectors into collection 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserted 1 documents into 'semantic_memory'.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   Processed 130/198 chunks...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:mem0.vector_stores.mongodb:Inserting 1 vectors into collection 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserted 1 documents into 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserting 1 vectors into collection 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserted 1 documents into 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserting 1 vectors into collection 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserted 1 documents into 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserting 1 vectors into collection 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserted 1 documents into 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserting 1 vectors into collection 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserted 1 documents into 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserting 1 vectors into collection 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserted 1 documents into 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserting 1 vectors into collection 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserted 1 documents into 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserting 1 vectors into collection 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserted 1 documents into 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserting 1 vectors into collection 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserted 1 documents into 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserting 1 vectors into collection 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserted 1 documents into 'semantic_memory'.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   Processed 140/198 chunks...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:mem0.vector_stores.mongodb:Inserting 1 vectors into collection 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserted 1 documents into 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserting 1 vectors into collection 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserted 1 documents into 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserting 1 vectors into collection 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserted 1 documents into 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserting 1 vectors into collection 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserted 1 documents into 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserting 1 vectors into collection 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserted 1 documents into 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserting 1 vectors into collection 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserted 1 documents into 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserting 1 vectors into collection 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserted 1 documents into 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserting 1 vectors into collection 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserted 1 documents into 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserting 1 vectors into collection 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserted 1 documents into 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserting 1 vectors into collection 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserted 1 documents into 'semantic_memory'.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   Processed 150/198 chunks...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:mem0.vector_stores.mongodb:Inserting 1 vectors into collection 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserted 1 documents into 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserting 1 vectors into collection 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserted 1 documents into 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserting 1 vectors into collection 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserted 1 documents into 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserting 1 vectors into collection 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserted 1 documents into 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserting 1 vectors into collection 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserted 1 documents into 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserting 1 vectors into collection 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserted 1 documents into 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserting 1 vectors into collection 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserted 1 documents into 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserting 1 vectors into collection 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserted 1 documents into 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserting 1 vectors into collection 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserted 1 documents into 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserting 1 vectors into collection 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserted 1 documents into 'semantic_memory'.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   Processed 160/198 chunks...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:mem0.vector_stores.mongodb:Inserting 1 vectors into collection 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserted 1 documents into 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserting 1 vectors into collection 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserted 1 documents into 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserting 1 vectors into collection 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserted 1 documents into 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserting 1 vectors into collection 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserted 1 documents into 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserting 1 vectors into collection 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserted 1 documents into 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserting 1 vectors into collection 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserted 1 documents into 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserting 1 vectors into collection 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserted 1 documents into 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserting 1 vectors into collection 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserted 1 documents into 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserting 1 vectors into collection 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserted 1 documents into 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserting 1 vectors into collection 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserted 1 documents into 'semantic_memory'.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   Processed 170/198 chunks...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:mem0.vector_stores.mongodb:Inserting 1 vectors into collection 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserted 1 documents into 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserting 1 vectors into collection 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserted 1 documents into 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserting 1 vectors into collection 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserted 1 documents into 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserting 1 vectors into collection 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserted 1 documents into 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserting 1 vectors into collection 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserted 1 documents into 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserting 1 vectors into collection 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserted 1 documents into 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserting 1 vectors into collection 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserted 1 documents into 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserting 1 vectors into collection 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserted 1 documents into 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserting 1 vectors into collection 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserted 1 documents into 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserting 1 vectors into collection 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserted 1 documents into 'semantic_memory'.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   Processed 180/198 chunks...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:mem0.vector_stores.mongodb:Inserting 1 vectors into collection 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserted 1 documents into 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserting 1 vectors into collection 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserted 1 documents into 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserting 1 vectors into collection 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserted 1 documents into 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserting 1 vectors into collection 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserted 1 documents into 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserting 1 vectors into collection 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserted 1 documents into 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserting 1 vectors into collection 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserted 1 documents into 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserting 1 vectors into collection 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserted 1 documents into 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserting 1 vectors into collection 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserted 1 documents into 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserting 1 vectors into collection 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserted 1 documents into 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserting 1 vectors into collection 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserted 1 documents into 'semantic_memory'.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   Processed 190/198 chunks...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:mem0.vector_stores.mongodb:Inserting 1 vectors into collection 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserted 1 documents into 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserting 1 vectors into collection 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserted 1 documents into 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserting 1 vectors into collection 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserted 1 documents into 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserting 1 vectors into collection 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserted 1 documents into 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserting 1 vectors into collection 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserted 1 documents into 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserting 1 vectors into collection 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserted 1 documents into 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserting 1 vectors into collection 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserted 1 documents into 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserting 1 vectors into collection 'semantic_memory'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserted 1 documents into 'semantic_memory'.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Ingestion complete!\n",
            "   - Successfully ingested: 198 chunks\n",
            "   - Failed: 0 chunks\n",
            "   - Total processed: 198/198\n"
          ]
        }
      ],
      "source": [
        "# Â∞ÜÂàÜÂùóÈÄê‰∏™ÊëÑÂÖ•ËØ≠‰πâËÆ∞ÂøÜÔºåÈÅøÂÖçË∂ÖÊó∂\n",
        "\n",
        "print(f\"üìÑ Ingesting {len(chunks)} chunks into semantic memory...\")\n",
        "\n",
        "successful_ingestions = 0\n",
        "failed_ingestions = 0\n",
        "\n",
        "for i, chunk in enumerate(chunks):\n",
        "    try:\n",
        "        # ÈÄê‰∏™Â§ÑÁêÜÊØè‰∏™ÂàÜÂùó\n",
        "        chunk_content = (chunk.get(\"value\") or {}).get(\"content\") or \"\"\n",
        "        if not chunk_content or len(chunk_content.strip()) < 50:\n",
        "            continue\n",
        "            \n",
        "        # ‰∏∫ËØ•ÂàÜÂùóÂàõÂª∫ÂçïÊù°Ê∂àÊÅØ\n",
        "        message = {\"role\": \"user\", \"content\": chunk_content}\n",
        "        \n",
        "        # Ê∑ªÂä†Âà∞ËØ≠‰πâËÆ∞ÂøÜ\n",
        "        result = semantic_memory.add([message], user_id=CURRENT_USER_ID, infer=False)\n",
        "        successful_ingestions += 1\n",
        "        \n",
        "        if (i + 1) % 10 == 0:  # ÊØèÂ§ÑÁêÜ 10 ‰∏™ÂàÜÂùóÊõ¥Êñ∞ËøõÂ∫¶\n",
        "            print(f\"   Processed {i + 1}/{len(chunks)} chunks...\")\n",
        "            \n",
        "    except Exception as e:\n",
        "        print(f\"   ‚ùå Failed to ingest chunk {i}: {str(e)[:100]}...\")\n",
        "        failed_ingestions += 1\n",
        "        continue\n",
        "\n",
        "print(f\"‚úÖ Ingestion complete!\")\n",
        "print(f\"   - Successfully ingested: {successful_ingestions} chunks\")\n",
        "print(f\"   - Failed: {failed_ingestions} chunks\")\n",
        "print(f\"   - Total processed: {successful_ingestions + failed_ingestions}/{len(chunks)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cbba1d97",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chat with AI (type 'exit' to quit)\n",
            "This AI has memory and can remember your conversation!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:mem0.vector_stores.mongodb:Retrieved document with ID 'c0c3c749-98eb-4d92-9620-8283735e0a66'.\n",
            "INFO:mem0.vector_stores.mongodb:Vector search completed. Found 0 documents.\n",
            "INFO:mem0.vector_stores.mongodb:Retrieved document with ID 'c0c3c749-98eb-4d92-9620-8283735e0a66'.\n",
            "INFO:mem0.vector_stores.mongodb:Vector search completed. Found 3 documents.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Memories:\n",
            "No relevant memories found.\n",
            "Semantic Memories:\n",
            "- [140] Dacheng Li, Rulin Shao, Anze Xie, Ying Sheng, Lianmin Zheng, Joseph Gonzalez, Ion Stoica,\n",
            "Xuezhe Ma, and Hao Zhang. How long can context length of open-source llms truly promise?\n",
            "In NeurIPS 2023 Workshop on Instruction Tuning and Instruction Following, 2023.\n",
            "[141] Mohit Shridhar, Xingdi Yuan, Marc-Alexandre C√¥t√©, Yonatan Bisk, Adam Trischler, and\n",
            "Matthew Hausknecht. Alfworld: Aligning text and embodied environments for interactive\n",
            "learning. arXiv preprint arXiv:2010.03768, 2020.\n",
            "[142] YunDa Tsai, Mingjie Liu, and Haoxing Ren. Rtlfixer: Automatically fixing rtl syntax errors\n",
            "with large language models. arXiv preprint arXiv:2311.16543, 2023.\n",
            "[143] Cheng Li, Ziang Leng, Chenxi Yan, Junyi Shen, Hao Wang, Weishi Mi, Yaying Fei, Xiaoyang\n",
            "Feng, Song Yan, HaoSheng Wang, et al. Chatharuhi: Reviving anime character in reality via\n",
            "large language model. arXiv preprint arXiv:2308.09597, 2023.\n",
            "[144] Dake Chen, Hanbin Wang, Yunhao Huo, Yuzhao Li, and Haoyang Zhang. Gamegpt: Multi-\n",
            "- [140] Dacheng Li, Rulin Shao, Anze Xie, Ying Sheng, Lianmin Zheng, Joseph Gonzalez, Ion Stoica,\n",
            "Xuezhe Ma, and Hao Zhang. How long can context length of open-source llms truly promise?\n",
            "In NeurIPS 2023 Workshop on Instruction Tuning and Instruction Following, 2023.\n",
            "[141] Mohit Shridhar, Xingdi Yuan, Marc-Alexandre C√¥t√©, Yonatan Bisk, Adam Trischler, and\n",
            "Matthew Hausknecht. Alfworld: Aligning text and embodied environments for interactive\n",
            "learning. arXiv preprint arXiv:2010.03768, 2020.\n",
            "[142] YunDa Tsai, Mingjie Liu, and Haoxing Ren. Rtlfixer: Automatically fixing rtl syntax errors\n",
            "with large language models. arXiv preprint arXiv:2311.16543, 2023.\n",
            "[143] Cheng Li, Ziang Leng, Chenxi Yan, Junyi Shen, Hao Wang, Weishi Mi, Yaying Fei, Xiaoyang\n",
            "Feng, Song Yan, HaoSheng Wang, et al. Chatharuhi: Reviving anime character in reality via\n",
            "large language model. arXiv preprint arXiv:2308.09597, 2023.\n",
            "[144] Dake Chen, Hanbin Wang, Yunhao Huo, Yuzhao Li, and Haoyang Zhang. Gamegpt: Multi-\n",
            "- [15] Peng Wang, Ningyu Zhang, Xin Xie, Yunzhi Yao, Bozhong Tian, Mengru Wang, Zekun Xi,\n",
            "Siyuan Cheng, Kangwei Liu, Guozhou Zheng, et al. Easyedit: An easy-to-use knowledge\n",
            "editing framework for large language models. arXiv preprint arXiv:2308.07269, 2023.\n",
            "[16] Zhangyin Feng, Weitao Ma, Weijiang Yu, Lei Huang, Haotian Wang, Qianglong Chen, Weihua\n",
            "Peng, Xiaocheng Feng, Bing Qin, et al. Trends in integration of knowledge and large language\n",
            "models: A survey and taxonomy of methods, benchmarks, and applications. arXiv preprint\n",
            "arXiv:2311.05876, 2023.\n",
            "[17] Ningyu Zhang, Yunzhi Yao, Bozhong Tian, Peng Wang, Shumin Deng, Mengru Wang, Zekun\n",
            "Xi, Shengyu Mao, Jintian Zhang, Yuansheng Ni, et al. A comprehensive study of knowledge\n",
            "editing for large language models. arXiv preprint arXiv:2401.01286, 2024.\n",
            "[18] Yujia Qin, Shengding Hu, Yankai Lin, Weize Chen, Ning Ding, Ganqu Cui, Zheni Zeng, Yufei\n",
            "Huang, Chaojun Xiao, Chi Han, et al. Tool learning with foundation models. arXiv preprint\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://newenglandhackopenai.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2025-03-01-preview \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://newenglandhackopenai.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2025-03-01-preview \"HTTP/1.1 200 OK\"\n",
            "INFO:mem0.memory.main:Total existing memories: 0\n",
            "INFO:mem0.vector_stores.mongodb:Retrieved document with ID 'c0c3c749-98eb-4d92-9620-8283735e0a66'.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "AI: Could you clarify your inquiry? Are you referring to a number, a context, or something specific?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:mem0.vector_stores.mongodb:Retrieved document with ID 'c0c3c749-98eb-4d92-9620-8283735e0a66'.\n",
            "INFO:mem0.vector_stores.mongodb:Vector search completed. Found 0 documents.\n",
            "INFO:mem0.vector_stores.mongodb:Retrieved document with ID 'c0c3c749-98eb-4d92-9620-8283735e0a66'.\n",
            "INFO:mem0.vector_stores.mongodb:Vector search completed. Found 3 documents.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Memories:\n",
            "No relevant memories found.\n",
            "Semantic Memories:\n",
            "- easily exceed the upper bound of the sequence length during LLM‚Äôs pretraining, which makes a\n",
            "truncation of memory necessary. Thus, it can lead to information loss due to the incompleteness\n",
            "of agent memory. Last but not least, it can lead to biases and unrobustness in LLM‚Äôs inference.\n",
            "Specifically, a previous research [ 120] has shown that, the positions of text segments in a long\n",
            "context can greatly affect their utilization, so the memory in the long-context prompt can not be\n",
            "treated equally and stably. All these drawbacks show the need to design extra memory modules for\n",
            "LLM-based agents, rather than straightforwardly concatenating all the information into a prompt.\n",
            "Recent Interactions. This method stores and maintains the most recently acquired memories using\n",
            "natural languages, thereby enhancing the efficiency of memory information utilization according\n",
            "to the Principle of Locality [121]. In task (B) of the example in Section 3.1, we can just remember\n",
            "- easily exceed the upper bound of the sequence length during LLM‚Äôs pretraining, which makes a\n",
            "truncation of memory necessary. Thus, it can lead to information loss due to the incompleteness\n",
            "of agent memory. Last but not least, it can lead to biases and unrobustness in LLM‚Äôs inference.\n",
            "Specifically, a previous research [ 120] has shown that, the positions of text segments in a long\n",
            "context can greatly affect their utilization, so the memory in the long-context prompt can not be\n",
            "treated equally and stably. All these drawbacks show the need to design extra memory modules for\n",
            "LLM-based agents, rather than straightforwardly concatenating all the information into a prompt.\n",
            "Recent Interactions. This method stores and maintains the most recently acquired memories using\n",
            "natural languages, thereby enhancing the efficiency of memory information utilization according\n",
            "to the Principle of Locality [121]. In task (B) of the example in Section 3.1, we can just remember\n",
            "- memory to store four types of information including (1) complete agent-environment interactions, (2)\n",
            "recent agent-environment interactions, (3) retrieved agent-environment interactions, and (4) external\n",
            "knowledge. In the former three methods, the memory leverages natural languages to describe the\n",
            "information within the agent-environment interaction loop. In the former three types, they record\n",
            "the information inside the agent-environment interaction loop, while the last type leverages natural\n",
            "languages to store information outside that loop.\n",
            "Complete Interactions. This method stores all the information of the agent-environment interaction\n",
            "history based on long-context strategies [116]. For the example in Section 3.1, the memory of the\n",
            "14\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://newenglandhackopenai.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2025-03-01-preview \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://newenglandhackopenai.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2025-03-01-preview \"HTTP/1.1 200 OK\"\n",
            "INFO:mem0.memory.main:Total existing memories: 0\n",
            "INFO:mem0.vector_stores.mongodb:Retrieved document with ID 'c0c3c749-98eb-4d92-9620-8283735e0a66'.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "AI: ÁúãËµ∑Êù•‰Ω†ÁöÑËÆ∞ÂøÜ‰∏≠Ê≤°ÊúâÂÖ≥‰∫é‰Ω†ÊúÄËøëÊèêÈóÆÁöÑÂÜÖÂÆπÁöÑËÆ∞ÂΩï„ÄÇ‰Ω†ËÉΩÂÖ∑‰ΩìËØ¥ËØ¥‰Ω†ÂàöÊâçÈóÆ‰∫Ü‰ªÄ‰πàÂêóÔºüÊàëÂèØ‰ª•Â∏ÆÂä©‰Ω†ÊâæÂà∞Á≠îÊ°àÊàñËÄÖÈáçÊñ∞ÂõûÁ≠î„ÄÇ\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:mem0.vector_stores.mongodb:Retrieved document with ID 'c0c3c749-98eb-4d92-9620-8283735e0a66'.\n",
            "INFO:mem0.vector_stores.mongodb:Vector search completed. Found 0 documents.\n",
            "INFO:mem0.vector_stores.mongodb:Retrieved document with ID 'c0c3c749-98eb-4d92-9620-8283735e0a66'.\n",
            "INFO:mem0.vector_stores.mongodb:Vector search completed. Found 3 documents.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Memories:\n",
            "No relevant memories found.\n",
            "Semantic Memories:\n",
            "- at https://github.com/nuster1128/LLM_Agent_Memory_Survey.\n",
            "Personal Assistant\n",
            "Please help me to explain \n",
            "‚ÄúLLM-based agent‚Äù.\n",
            "A LLM-based agent is a \n",
            "type of artificial ‚Ä¶‚Ä¶\n",
            "In which scenarios does it \n",
            "have applications?\n",
            "Personal assistant, game,\n",
            "code generation, ‚Ä¶‚Ä¶\n",
            "(Knowledge) According to the previous works, large \n",
            "language model based agents refer to artificial ‚Ä¶‚Ä¶\n",
            "(Context) The current topic is LLM-based agent. ‚ÄúIt‚Äù \n",
            "refers to LLM-based agents in this conversation.\n",
            "Social Simulation\n",
            " I'm a compassionate physician \n",
            "specializing in cardiology, \n",
            "committed to improving patients' \n",
            "heart health and well-being.\n",
            "I'm a skilled nurse dedicated to \n",
            "patient care, ensuring comfort and \n",
            "supporting health with empathy \n",
            "and expertise.\n",
            "Role-playing\n",
            "I' m a Smurf, and \n",
            "Smurfs are us!\n",
            "Have you ever had \n",
            "a dream?\n",
            "Magic is all \n",
            "around the us!\n",
            "Jarvis, we must \n",
            "first learn to run!\n",
            "Wubalubadu.\n",
            "Bdub Wuckoop.\n",
            "I' m Batman, the \n",
            "lights of city.\n",
            "[Iron Man] My name is Iron Man, \n",
            "also known as Tony Stark. I am\n",
            "- at https://github.com/nuster1128/LLM_Agent_Memory_Survey.\n",
            "Personal Assistant\n",
            "Please help me to explain \n",
            "‚ÄúLLM-based agent‚Äù.\n",
            "A LLM-based agent is a \n",
            "type of artificial ‚Ä¶‚Ä¶\n",
            "In which scenarios does it \n",
            "have applications?\n",
            "Personal assistant, game,\n",
            "code generation, ‚Ä¶‚Ä¶\n",
            "(Knowledge) According to the previous works, large \n",
            "language model based agents refer to artificial ‚Ä¶‚Ä¶\n",
            "(Context) The current topic is LLM-based agent. ‚ÄúIt‚Äù \n",
            "refers to LLM-based agents in this conversation.\n",
            "Social Simulation\n",
            " I'm a compassionate physician \n",
            "specializing in cardiology, \n",
            "committed to improving patients' \n",
            "heart health and well-being.\n",
            "I'm a skilled nurse dedicated to \n",
            "patient care, ensuring comfort and \n",
            "supporting health with empathy \n",
            "and expertise.\n",
            "Role-playing\n",
            "I' m a Smurf, and \n",
            "Smurfs are us!\n",
            "Have you ever had \n",
            "a dream?\n",
            "Magic is all \n",
            "around the us!\n",
            "Jarvis, we must \n",
            "first learn to run!\n",
            "Wubalubadu.\n",
            "Bdub Wuckoop.\n",
            "I' m Batman, the \n",
            "lights of city.\n",
            "[Iron Man] My name is Iron Man, \n",
            "also known as Tony Stark. I am\n",
            "- iao Peng, Jiaming Yang, Xiyao Xiao, et al. Characterglm: Customizing chinese conversational\n",
            "ai characters with large language models. arXiv preprint arXiv:2311.16832, 2023.\n",
            "[148] Zhao Kaiya, Michelangelo Naim, Jovana Kondic, Manuel Cortes, Jiaxin Ge, Shuying Luo,\n",
            "Guangyu Robert Yang, and Andrew Ahn. Lyfe agents: Generative agents for low-cost real-time\n",
            "social interactions. arXiv preprint arXiv:2310.02172, 2023.\n",
            "[149] Junjie Zhang, Yupeng Hou, Ruobing Xie, Wenqi Sun, Julian McAuley, Wayne Xin Zhao, Leyu\n",
            "Lin, and Ji-Rong Wen. Agentcf: Collaborative learning with autonomous language agents for\n",
            "recommender systems. arXiv preprint arXiv:2310.09233, 2023.\n",
            "[150] Wenyue Hua, Lizhou Fan, Lingyao Li, Kai Mei, Jianchao Ji, Yingqiang Ge, Libby Hemphill,\n",
            "and Yongfeng Zhang. War and peace (waragent): Large language model-based multi-agent\n",
            "simulation of world wars. arXiv preprint arXiv:2311.17227, 2023.\n",
            "[151] Haochun Wang, Sendong Zhao, Zewen Qiang, Zijian Li, Nuwa Xi, Yanrui Du, MuZhen Cai,\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://newenglandhackopenai.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2025-03-01-preview \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://newenglandhackopenai.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2025-03-01-preview \"HTTP/1.1 200 OK\"\n",
            "INFO:mem0.vector_stores.mongodb:Vector search completed. Found 0 documents.\n",
            "INFO:mem0.memory.main:Total existing memories: 0\n",
            "INFO:httpx:HTTP Request: POST https://newenglandhackopenai.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2025-03-01-preview \"HTTP/1.1 200 OK\"\n",
            "INFO:mem0.memory.main:{'id': '0', 'text': 'Áî®Êà∑ÁöÑÂêçÂ≠óÊòØuser001', 'event': 'ADD'}\n",
            "INFO:mem0.vector_stores.mongodb:Inserting 1 vectors into collection 'extracted_memories'.\n",
            "INFO:mem0.vector_stores.mongodb:Inserted 1 documents into 'extracted_memories'.\n",
            "INFO:mem0.vector_stores.mongodb:Retrieved document with ID 'c0c3c749-98eb-4d92-9620-8283735e0a66'.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "AI: ‰Ω†Â•ΩÔºåuser001ÔºÅÊúâ‰ªÄ‰πàÂèØ‰ª•Â∏ÆÂä©‰Ω†ÁöÑÂë¢Ôºü üòä\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:mem0.vector_stores.mongodb:Retrieved document with ID 'c0c3c749-98eb-4d92-9620-8283735e0a66'.\n",
            "INFO:mem0.vector_stores.mongodb:Vector search completed. Found 1 documents.\n",
            "INFO:mem0.vector_stores.mongodb:Retrieved document with ID 'c0c3c749-98eb-4d92-9620-8283735e0a66'.\n",
            "INFO:mem0.vector_stores.mongodb:Vector search completed. Found 3 documents.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Memories:\n",
            "- Áî®Êà∑ÁöÑÂêçÂ≠óÊòØuser001\n",
            "Semantic Memories:\n",
            "- easily exceed the upper bound of the sequence length during LLM‚Äôs pretraining, which makes a\n",
            "truncation of memory necessary. Thus, it can lead to information loss due to the incompleteness\n",
            "of agent memory. Last but not least, it can lead to biases and unrobustness in LLM‚Äôs inference.\n",
            "Specifically, a previous research [ 120] has shown that, the positions of text segments in a long\n",
            "context can greatly affect their utilization, so the memory in the long-context prompt can not be\n",
            "treated equally and stably. All these drawbacks show the need to design extra memory modules for\n",
            "LLM-based agents, rather than straightforwardly concatenating all the information into a prompt.\n",
            "Recent Interactions. This method stores and maintains the most recently acquired memories using\n",
            "natural languages, thereby enhancing the efficiency of memory information utilization according\n",
            "to the Principle of Locality [121]. In task (B) of the example in Section 3.1, we can just remember\n",
            "- easily exceed the upper bound of the sequence length during LLM‚Äôs pretraining, which makes a\n",
            "truncation of memory necessary. Thus, it can lead to information loss due to the incompleteness\n",
            "of agent memory. Last but not least, it can lead to biases and unrobustness in LLM‚Äôs inference.\n",
            "Specifically, a previous research [ 120] has shown that, the positions of text segments in a long\n",
            "context can greatly affect their utilization, so the memory in the long-context prompt can not be\n",
            "treated equally and stably. All these drawbacks show the need to design extra memory modules for\n",
            "LLM-based agents, rather than straightforwardly concatenating all the information into a prompt.\n",
            "Recent Interactions. This method stores and maintains the most recently acquired memories using\n",
            "natural languages, thereby enhancing the efficiency of memory information utilization according\n",
            "to the Principle of Locality [121]. In task (B) of the example in Section 3.1, we can just remember\n",
            "- Discussion. The inside-trial information is the most obvious and intuitive source that should be\n",
            "leveraged to construct the agent‚Äôs memory since it is highly relevant to the current task that the agent\n",
            "has to accomplish. However, relying solely on inside-trial information may prevent the agent from\n",
            "accumulating valuable knowledge from various tasks and learning more generalizable information.\n",
            "Thus, many studies also explore how to effectively utilize the information across different tasks to\n",
            "build the memory module, which is detailed in the following sections.\n",
            "5.1.2 Cross-trial Information\n",
            "For LLM-based agents, the information accumulated across multiple trials in the environment is also\n",
            "a crucial part of the memory, typically including successful and failed actions and their insights, such\n",
            "as failure reasons, common action patterns to succeed, and so on.\n",
            "Representative Studies. One of the most prominent studies is Reflexion [5], which proposes verbal\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://newenglandhackopenai.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2025-03-01-preview \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://newenglandhackopenai.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2025-03-01-preview \"HTTP/1.1 200 OK\"\n",
            "INFO:mem0.vector_stores.mongodb:Vector search completed. Found 1 documents.\n",
            "INFO:mem0.memory.main:Total existing memories: 1\n",
            "INFO:httpx:HTTP Request: POST https://newenglandhackopenai.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2025-03-01-preview \"HTTP/1.1 200 OK\"\n",
            "INFO:mem0.memory.main:{'id': '0', 'text': 'Áî®Êà∑ÁöÑÂêçÂ≠óÊòØuser001', 'event': 'NONE'}\n",
            "INFO:mem0.memory.main:NOOP for Memory.\n",
            "INFO:mem0.vector_stores.mongodb:Retrieved document with ID 'c0c3c749-98eb-4d92-9620-8283735e0a66'.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "AI: ‰Ω†Âπ∂Êú™Âú®‰πãÂâçÁöÑÂØπËØù‰∏≠Áïô‰∏ãÂÖ∑‰ΩìÁöÑÈóÆÈ¢òËÆ∞ÂΩïÔºåÂè™ÊòØÊ∂âÂèä‰∫Ü‰∏Ä‰∫õÂÖ≥‰∫éËÆ∞ÂøÜÊ®°ÂùóÂíåLLMÁöÑËÆ®ËÆ∫„ÄÇÂ¶ÇÊûúÈúÄË¶ÅÔºå‰Ω†ÂèØ‰ª•ÈáçËø∞‰Ω†ÁöÑÈóÆÈ¢òÔºåÊàë‰ºöÂ∞ΩÂäõÂõûÁ≠îÔºÅ\n",
            "AI: Please enter a message.\n",
            "AI: Please enter a message.\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[32]\u001b[39m\u001b[32m, line 74\u001b[39m\n\u001b[32m     71\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[32m     73\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m74\u001b[39m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[32]\u001b[39m\u001b[32m, line 58\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     56\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     57\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m58\u001b[39m         user_input = \u001b[38;5;28;43minput\u001b[39;49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43mYou: \u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m.strip()\n\u001b[32m     59\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m user_input.lower() == \u001b[33m'\u001b[39m\u001b[33mexit\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m     60\u001b[39m             \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mGoodbye!\u001b[39m\u001b[33m\"\u001b[39m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Demo/agent_memory_demo/venv/lib/python3.12/site-packages/ipykernel/kernelbase.py:1396\u001b[39m, in \u001b[36mKernel.raw_input\u001b[39m\u001b[34m(self, prompt)\u001b[39m\n\u001b[32m   1394\u001b[39m     msg = \u001b[33m\"\u001b[39m\u001b[33mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1395\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(msg)\n\u001b[32m-> \u001b[39m\u001b[32m1396\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_input_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1397\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1398\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_shell_context_var\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_shell_parent_ident\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1399\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_parent\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mshell\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1400\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpassword\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1401\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Demo/agent_memory_demo/venv/lib/python3.12/site-packages/ipykernel/kernelbase.py:1441\u001b[39m, in \u001b[36mKernel._input_request\u001b[39m\u001b[34m(self, prompt, ident, parent, password)\u001b[39m\n\u001b[32m   1438\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[32m   1439\u001b[39m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[32m   1440\u001b[39m     msg = \u001b[33m\"\u001b[39m\u001b[33mInterrupted by user\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1441\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1442\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m   1443\u001b[39m     \u001b[38;5;28mself\u001b[39m.log.warning(\u001b[33m\"\u001b[39m\u001b[33mInvalid Message:\u001b[39m\u001b[33m\"\u001b[39m, exc_info=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: Interrupted by user"
          ]
        }
      ],
      "source": [
        "def chat_with_memories(message: str, user_id: str = None) -> str:\n",
        "    \"\"\"\n",
        "    Chat function that retrieves relevant memories and generates responses.\n",
        "    \n",
        "    Args:\n",
        "        message (str): User's input message\n",
        "        user_id (str): User identifier for memory retrieval (default: CURRENT_USER_ID)\n",
        "        \n",
        "    Returns:\n",
        "        str: Assistant's response based on memories and query\n",
        "    \"\"\"\n",
        "    if user_id is None:\n",
        "        user_id = CURRENT_USER_ID\n",
        "    if not message or not message.strip():\n",
        "        return \"Please enter a message.\"\n",
        "    try:\n",
        "        # Retrieve relevant memories using the fixed search method\n",
        "        relevant_memories = memory.search(query=message, user_id=user_id, limit=3)\n",
        "        relevant_memories_from_semantic_memory = semantic_memory.search(query=message, user_id=user_id, limit=3)\n",
        "        \n",
        "        # Format memories for display and context (defensive: .get(\"results\", []) to avoid KeyError)\n",
        "        memories_str = \"\\n\".join(f\"- {entry['memory']}\" for entry in relevant_memories.get(\"results\", []))\n",
        "        memories_str_from_semantic_memory = \"\\n\".join(f\"- {entry['memory']}\" for entry in relevant_memories_from_semantic_memory.get(\"results\", []))\n",
        "        print(\"Memories:\")\n",
        "        print(memories_str if memories_str else \"No relevant memories found.\")\n",
        "        # print(\"Semantic Memories:\")\n",
        "        # print(memories_str_from_semantic_memory if memories_str_from_semantic_memory else \"No relevant semantic memories found.\")\n",
        "\n",
        "        # Generate Assistant response with memory context\n",
        "        system_prompt = f\"You are a helpful AI. Answer the question based on the query and memories.\\nUser Memories:\\n{memories_str}\\nSemantic Memories:\\n{memories_str_from_semantic_memory}\"\n",
        "        messages = [{\"role\": \"system\", \"content\": system_prompt}, {\"role\": \"user\", \"content\": message}]\n",
        "        \n",
        "        response = openai_client.chat.completions.create(model=\"gpt-4o\", messages=messages)\n",
        "        assistant_response = response.choices[0].message.content\n",
        "\n",
        "        # Create new memories from the conversation (same user_id ‰øùËØÅ‰∏ãÊ¨°ËÉΩÊ£ÄÁ¥¢Âà∞)\n",
        "        messages.append({\"role\": \"assistant\", \"content\": assistant_response})\n",
        "        try:\n",
        "            memory.add(messages, user_id=user_id)\n",
        "        except Exception as add_err:\n",
        "            print(f\"Warning: ËÆ∞ÂøÜÂÜôÂÖ•Â§±Ë¥•Ôºå‰∏ãÊ¨°ÂèØËÉΩÊó†Ê≥ïËÆ∞‰ΩèÊú¨Ê¨°ÂØπËØù: {add_err}\")\n",
        "\n",
        "        return assistant_response\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"Error in chat_with_memories: {e}\")\n",
        "        # Fallback response without memory\n",
        "        messages = [{\"role\": \"user\", \"content\": message}]\n",
        "        response = openai_client.chat.completions.create(model=\"gpt-4o\", messages=messages)\n",
        "        return response.choices[0].message.content\n",
        "\n",
        "def main():\n",
        "    \"\"\"Interactive chat loop (for terminal; in Jupyter use chat_with_memories('your question') in a cell)\"\"\"\n",
        "    print(\"Chat with AI (type 'exit' to quit)\")\n",
        "    print(\"This AI has memory and can remember your conversation!\")\n",
        "    try:\n",
        "        while True:\n",
        "            user_input = input(\"\\nYou: \").strip()\n",
        "            if user_input.lower() == 'exit':\n",
        "                print(\"Goodbye!\")\n",
        "                break\n",
        "            try:\n",
        "                response = chat_with_memories(user_input)\n",
        "                print(f\"AI: {response}\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error: {e}\")\n",
        "    except Exception as e:\n",
        "        if \"StdinNotImplementedError\" in type(e).__name__ or \"input\" in str(e).lower():\n",
        "            print(\"Notebook: run chat_with_memories('your question') in a cell instead.\")\n",
        "        else:\n",
        "            raise\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "c521daa6",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:mem0.vector_stores.mongodb:Retrieved document with ID 'c0c3c749-98eb-4d92-9620-8283735e0a66'.\n",
            "INFO:mem0.vector_stores.mongodb:Vector search completed. Found 1 documents.\n"
          ]
        }
      ],
      "source": [
        "coffee_search = memory.search(query=\"coffee brewing methods and preferences\", user_id=CURRENT_USER_ID, limit=5)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "e79a22cd",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'results': [{'updated_at': None,\n",
              "   'hash': None,\n",
              "   'user_id': 'user-123',\n",
              "   'agent_id': None,\n",
              "   'data': 'Áî®Êà∑ÁöÑÂêçÂ≠óÊòØuser001',\n",
              "   'role': None,\n",
              "   'created_at': None,\n",
              "   'actor_id': None,\n",
              "   'run_id': None,\n",
              "   'id': '200770eb-98a6-403d-836b-8dcdb46c3f90',\n",
              "   'score': 0.614314079284668,\n",
              "   'memory': 'Áî®Êà∑ÁöÑÂêçÂ≠óÊòØuser001',\n",
              "   'metadata': {}}]}"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "coffee_search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "49ebb327",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:mem0.vector_stores.mongodb:Retrieved document with ID 'c0c3c749-98eb-4d92-9620-8283735e0a66'.\n",
            "INFO:mem0.vector_stores.mongodb:Vector search completed. Found 5 documents.\n"
          ]
        }
      ],
      "source": [
        "paper_search = semantic_memory.search(query=\"What is the main idea of the paper?\", user_id=CURRENT_USER_ID, limit=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "c400608e",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'results': [{'updated_at': None,\n",
              "   'hash': None,\n",
              "   'user_id': 'user-123',\n",
              "   'agent_id': None,\n",
              "   'data': 'be stored in the same group. In SCM [98], it designs a memory controller to decide when to execute\\nthe operations. The controller serves as a guide for the whole memory module. In MemGPT [100],\\nthe memory writing is entirely self-directed. The agents can autonomously update the memory based\\non the contexts. In MemoChat [94], the agents summarize each conversation segment by abstracting\\nthe mainly discussed topics and storing them as keys for indexing memory pieces.\\nDiscussion. Previous research indicates that designing the strategy of information extraction during\\nthe memory writing operation is vital [94]. This is because the original information is commonly\\nlengthy and noisy. Besides, different environments may provide various forms of feedback, and how\\nto extract and represent the information as memory is also significant for memory writing.\\n5.3.2 Memory Management\\nFor human beings, memory information is constantly processed and abstracted in the brains. The',\n",
              "   'role': 'user',\n",
              "   'created_at': None,\n",
              "   'actor_id': None,\n",
              "   'run_id': None,\n",
              "   'id': 'a6e6a9ab-3cb8-4b5b-a130-0698a37c3d57',\n",
              "   'score': 0.7605617046356201,\n",
              "   'memory': 'be stored in the same group. In SCM [98], it designs a memory controller to decide when to execute\\nthe operations. The controller serves as a guide for the whole memory module. In MemGPT [100],\\nthe memory writing is entirely self-directed. The agents can autonomously update the memory based\\non the contexts. In MemoChat [94], the agents summarize each conversation segment by abstracting\\nthe mainly discussed topics and storing them as keys for indexing memory pieces.\\nDiscussion. Previous research indicates that designing the strategy of information extraction during\\nthe memory writing operation is vital [94]. This is because the original information is commonly\\nlengthy and noisy. Besides, different environments may provide various forms of feedback, and how\\nto extract and represent the information as memory is also significant for memory writing.\\n5.3.2 Memory Management\\nFor human beings, memory information is constantly processed and abstracted in the brains. The',\n",
              "   'metadata': {}},\n",
              "  {'updated_at': None,\n",
              "   'hash': None,\n",
              "   'user_id': 'user-123',\n",
              "   'agent_id': None,\n",
              "   'data': 'be stored in the same group. In SCM [98], it designs a memory controller to decide when to execute\\nthe operations. The controller serves as a guide for the whole memory module. In MemGPT [100],\\nthe memory writing is entirely self-directed. The agents can autonomously update the memory based\\non the contexts. In MemoChat [94], the agents summarize each conversation segment by abstracting\\nthe mainly discussed topics and storing them as keys for indexing memory pieces.\\nDiscussion. Previous research indicates that designing the strategy of information extraction during\\nthe memory writing operation is vital [94]. This is because the original information is commonly\\nlengthy and noisy. Besides, different environments may provide various forms of feedback, and how\\nto extract and represent the information as memory is also significant for memory writing.\\n5.3.2 Memory Management\\nFor human beings, memory information is constantly processed and abstracted in the brains. The',\n",
              "   'role': 'user',\n",
              "   'created_at': None,\n",
              "   'actor_id': None,\n",
              "   'run_id': None,\n",
              "   'id': 'e37bb366-d18e-413f-bd23-1843bd12d74d',\n",
              "   'score': 0.7605617046356201,\n",
              "   'memory': 'be stored in the same group. In SCM [98], it designs a memory controller to decide when to execute\\nthe operations. The controller serves as a guide for the whole memory module. In MemGPT [100],\\nthe memory writing is entirely self-directed. The agents can autonomously update the memory based\\non the contexts. In MemoChat [94], the agents summarize each conversation segment by abstracting\\nthe mainly discussed topics and storing them as keys for indexing memory pieces.\\nDiscussion. Previous research indicates that designing the strategy of information extraction during\\nthe memory writing operation is vital [94]. This is because the original information is commonly\\nlengthy and noisy. Besides, different environments may provide various forms of feedback, and how\\nto extract and represent the information as memory is also significant for memory writing.\\n5.3.2 Memory Management\\nFor human beings, memory information is constantly processed and abstracted in the brains. The',\n",
              "   'metadata': {}},\n",
              "  {'updated_at': None,\n",
              "   'hash': None,\n",
              "   'user_id': 'user-123',\n",
              "   'agent_id': None,\n",
              "   'data': '[15] Peng Wang, Ningyu Zhang, Xin Xie, Yunzhi Yao, Bozhong Tian, Mengru Wang, Zekun Xi,\\nSiyuan Cheng, Kangwei Liu, Guozhou Zheng, et al. Easyedit: An easy-to-use knowledge\\nediting framework for large language models. arXiv preprint arXiv:2308.07269, 2023.\\n[16] Zhangyin Feng, Weitao Ma, Weijiang Yu, Lei Huang, Haotian Wang, Qianglong Chen, Weihua\\nPeng, Xiaocheng Feng, Bing Qin, et al. Trends in integration of knowledge and large language\\nmodels: A survey and taxonomy of methods, benchmarks, and applications. arXiv preprint\\narXiv:2311.05876, 2023.\\n[17] Ningyu Zhang, Yunzhi Yao, Bozhong Tian, Peng Wang, Shumin Deng, Mengru Wang, Zekun\\nXi, Shengyu Mao, Jintian Zhang, Yuansheng Ni, et al. A comprehensive study of knowledge\\nediting for large language models. arXiv preprint arXiv:2401.01286, 2024.\\n[18] Yujia Qin, Shengding Hu, Yankai Lin, Weize Chen, Ning Ding, Ganqu Cui, Zheni Zeng, Yufei\\nHuang, Chaojun Xiao, Chi Han, et al. Tool learning with foundation models. arXiv preprint',\n",
              "   'role': 'user',\n",
              "   'created_at': None,\n",
              "   'actor_id': None,\n",
              "   'run_id': None,\n",
              "   'id': 'c2884d0d-6b3f-406e-9965-0313de9f3921',\n",
              "   'score': 0.7597818374633789,\n",
              "   'memory': '[15] Peng Wang, Ningyu Zhang, Xin Xie, Yunzhi Yao, Bozhong Tian, Mengru Wang, Zekun Xi,\\nSiyuan Cheng, Kangwei Liu, Guozhou Zheng, et al. Easyedit: An easy-to-use knowledge\\nediting framework for large language models. arXiv preprint arXiv:2308.07269, 2023.\\n[16] Zhangyin Feng, Weitao Ma, Weijiang Yu, Lei Huang, Haotian Wang, Qianglong Chen, Weihua\\nPeng, Xiaocheng Feng, Bing Qin, et al. Trends in integration of knowledge and large language\\nmodels: A survey and taxonomy of methods, benchmarks, and applications. arXiv preprint\\narXiv:2311.05876, 2023.\\n[17] Ningyu Zhang, Yunzhi Yao, Bozhong Tian, Peng Wang, Shumin Deng, Mengru Wang, Zekun\\nXi, Shengyu Mao, Jintian Zhang, Yuansheng Ni, et al. A comprehensive study of knowledge\\nediting for large language models. arXiv preprint arXiv:2401.01286, 2024.\\n[18] Yujia Qin, Shengding Hu, Yankai Lin, Weize Chen, Ning Ding, Ganqu Cui, Zheni Zeng, Yufei\\nHuang, Chaojun Xiao, Chi Han, et al. Tool learning with foundation models. arXiv preprint',\n",
              "   'metadata': {}},\n",
              "  {'updated_at': None,\n",
              "   'hash': None,\n",
              "   'user_id': 'user-123',\n",
              "   'agent_id': None,\n",
              "   'data': '[15] Peng Wang, Ningyu Zhang, Xin Xie, Yunzhi Yao, Bozhong Tian, Mengru Wang, Zekun Xi,\\nSiyuan Cheng, Kangwei Liu, Guozhou Zheng, et al. Easyedit: An easy-to-use knowledge\\nediting framework for large language models. arXiv preprint arXiv:2308.07269, 2023.\\n[16] Zhangyin Feng, Weitao Ma, Weijiang Yu, Lei Huang, Haotian Wang, Qianglong Chen, Weihua\\nPeng, Xiaocheng Feng, Bing Qin, et al. Trends in integration of knowledge and large language\\nmodels: A survey and taxonomy of methods, benchmarks, and applications. arXiv preprint\\narXiv:2311.05876, 2023.\\n[17] Ningyu Zhang, Yunzhi Yao, Bozhong Tian, Peng Wang, Shumin Deng, Mengru Wang, Zekun\\nXi, Shengyu Mao, Jintian Zhang, Yuansheng Ni, et al. A comprehensive study of knowledge\\nediting for large language models. arXiv preprint arXiv:2401.01286, 2024.\\n[18] Yujia Qin, Shengding Hu, Yankai Lin, Weize Chen, Ning Ding, Ganqu Cui, Zheni Zeng, Yufei\\nHuang, Chaojun Xiao, Chi Han, et al. Tool learning with foundation models. arXiv preprint',\n",
              "   'role': 'user',\n",
              "   'created_at': None,\n",
              "   'actor_id': None,\n",
              "   'run_id': None,\n",
              "   'id': '1e81fde2-879b-43f7-8d83-028b8f717c04',\n",
              "   'score': 0.7597818374633789,\n",
              "   'memory': '[15] Peng Wang, Ningyu Zhang, Xin Xie, Yunzhi Yao, Bozhong Tian, Mengru Wang, Zekun Xi,\\nSiyuan Cheng, Kangwei Liu, Guozhou Zheng, et al. Easyedit: An easy-to-use knowledge\\nediting framework for large language models. arXiv preprint arXiv:2308.07269, 2023.\\n[16] Zhangyin Feng, Weitao Ma, Weijiang Yu, Lei Huang, Haotian Wang, Qianglong Chen, Weihua\\nPeng, Xiaocheng Feng, Bing Qin, et al. Trends in integration of knowledge and large language\\nmodels: A survey and taxonomy of methods, benchmarks, and applications. arXiv preprint\\narXiv:2311.05876, 2023.\\n[17] Ningyu Zhang, Yunzhi Yao, Bozhong Tian, Peng Wang, Shumin Deng, Mengru Wang, Zekun\\nXi, Shengyu Mao, Jintian Zhang, Yuansheng Ni, et al. A comprehensive study of knowledge\\nediting for large language models. arXiv preprint arXiv:2401.01286, 2024.\\n[18] Yujia Qin, Shengding Hu, Yankai Lin, Weize Chen, Ning Ding, Ganqu Cui, Zheni Zeng, Yufei\\nHuang, Chaojun Xiao, Chi Han, et al. Tool learning with foundation models. arXiv preprint',\n",
              "   'metadata': {}},\n",
              "  {'updated_at': None,\n",
              "   'hash': None,\n",
              "   'user_id': 'user-123',\n",
              "   'agent_id': None,\n",
              "   'data': 'model editing at scale. arXiv preprint arXiv:2110.11309, 2021.\\n[135] Shengyu Mao, Ningyu Zhang, Xiaohan Wang, Mengru Wang, Yunzhi Yao, Yong Jiang,\\nPengjun Xie, Fei Huang, and Huajun Chen. Editing personality for large language models.\\n2023.\\n[136] Jun-Yu Ma, Jia-Chen Gu, Ningyu Zhang, and Zhen-Hua Ling. Neighboring perturbations of\\nknowledge editing on large language models. arXiv preprint arXiv:2401.17623, 2024.\\n[137] Mengru Wang, Ningyu Zhang, Ziwen Xu, Zekun Xi, Shumin Deng, Yunzhi Yao, Qishen\\nZhang, Linyi Yang, Jindong Wang, and Huajun Chen. Detoxifying large language models via\\nknowledge editing. arXiv preprint arXiv:2403.14472, 2024.\\n[138] Uri Shaham, Maor Ivgi, Avia Efrat, Jonathan Berant, and Omer Levy. Zeroscrolls: A zero-shot\\nbenchmark for long text understanding. arXiv preprint arXiv:2305.14196, 2023.\\n[139] Yushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Jiankai Tang, Zhidian Huang, Zhengxiao',\n",
              "   'role': 'user',\n",
              "   'created_at': None,\n",
              "   'actor_id': None,\n",
              "   'run_id': None,\n",
              "   'id': '88c34425-f5a0-4327-a4fb-739aa39036f9',\n",
              "   'score': 0.7594928741455078,\n",
              "   'memory': 'model editing at scale. arXiv preprint arXiv:2110.11309, 2021.\\n[135] Shengyu Mao, Ningyu Zhang, Xiaohan Wang, Mengru Wang, Yunzhi Yao, Yong Jiang,\\nPengjun Xie, Fei Huang, and Huajun Chen. Editing personality for large language models.\\n2023.\\n[136] Jun-Yu Ma, Jia-Chen Gu, Ningyu Zhang, and Zhen-Hua Ling. Neighboring perturbations of\\nknowledge editing on large language models. arXiv preprint arXiv:2401.17623, 2024.\\n[137] Mengru Wang, Ningyu Zhang, Ziwen Xu, Zekun Xi, Shumin Deng, Yunzhi Yao, Qishen\\nZhang, Linyi Yang, Jindong Wang, and Huajun Chen. Detoxifying large language models via\\nknowledge editing. arXiv preprint arXiv:2403.14472, 2024.\\n[138] Uri Shaham, Maor Ivgi, Avia Efrat, Jonathan Berant, and Omer Levy. Zeroscrolls: A zero-shot\\nbenchmark for long text understanding. arXiv preprint arXiv:2305.14196, 2023.\\n[139] Yushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Jiankai Tang, Zhidian Huang, Zhengxiao',\n",
              "   'metadata': {}}]}"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "paper_search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "id": "36152685",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:mem0.vector_stores.mongodb:Retrieved document with ID 'c0c3c749-98eb-4d92-9620-8283735e0a66'.\n",
            "INFO:mem0.vector_stores.mongodb:Vector search completed. Found 3 documents.\n",
            "INFO:mem0.vector_stores.mongodb:Retrieved document with ID 'c0c3c749-98eb-4d92-9620-8283735e0a66'.\n",
            "INFO:mem0.vector_stores.mongodb:Vector search completed. Found 3 documents.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Memories:\n",
            "- Áî®Êà∑ÂñúÊ¨¢ÂêÉËãπÊûú\n",
            "- ÂêçÂ≠óÂè´Âë®Êñå\n",
            "- Áî®Êà∑ÁöÑÂêçÂ≠óÊòØuser001\n",
            "Semantic Memories:\n",
            "- You may like this blue dress. It is \n",
            "of good quality and great price.\n",
            "Would you like to buy a \n",
            "waistband for your dress?\n",
            "Great! I like this bule one. \n",
            "I will buy it for the party.\n",
            "Medicine\n",
            "David, a 38-year-old male with a history of allergies and sinus \n",
            "infections, has a family history of diabetes and hypertension. As \n",
            "a smoker of about one pack a day and an occasional drinker, \n",
            "his lifestyle choices may contribute to his health risks. After \n",
            "traveling to a tropical country where mosquito-borne illnesses \n",
            "are prevalent, he has experienced symptoms such as mild \n",
            "fatigue, headache, and muscle aches for the past week. \n",
            "For three days, I‚Äôve had a \n",
            "fever ranging from \n",
            "100.5¬∞F to 102¬∞F, a rash \n",
            "on my limbs, joint pain \n",
            "and swelling, especially \n",
            "in my hands, episodes of \n",
            "diarrhea, abdominal pain, \n",
            "and nausea, leading to a \n",
            "loss of appetite.\n",
            "The patient‚Äòs travel history and \n",
            "symptoms suggest dengue fever, \n",
            "a mosquito-transmitted illness.\n",
            "Finance\n",
            "- You may like this blue dress. It is \n",
            "of good quality and great price.\n",
            "Would you like to buy a \n",
            "waistband for your dress?\n",
            "Great! I like this bule one. \n",
            "I will buy it for the party.\n",
            "Medicine\n",
            "David, a 38-year-old male with a history of allergies and sinus \n",
            "infections, has a family history of diabetes and hypertension. As \n",
            "a smoker of about one pack a day and an occasional drinker, \n",
            "his lifestyle choices may contribute to his health risks. After \n",
            "traveling to a tropical country where mosquito-borne illnesses \n",
            "are prevalent, he has experienced symptoms such as mild \n",
            "fatigue, headache, and muscle aches for the past week. \n",
            "For three days, I‚Äôve had a \n",
            "fever ranging from \n",
            "100.5¬∞F to 102¬∞F, a rash \n",
            "on my limbs, joint pain \n",
            "and swelling, especially \n",
            "in my hands, episodes of \n",
            "diarrhea, abdominal pain, \n",
            "and nausea, leading to a \n",
            "loss of appetite.\n",
            "The patient‚Äòs travel history and \n",
            "symptoms suggest dengue fever, \n",
            "a mosquito-transmitted illness.\n",
            "Finance\n",
            "- at https://github.com/nuster1128/LLM_Agent_Memory_Survey.\n",
            "Personal Assistant\n",
            "Please help me to explain \n",
            "‚ÄúLLM-based agent‚Äù.\n",
            "A LLM-based agent is a \n",
            "type of artificial ‚Ä¶‚Ä¶\n",
            "In which scenarios does it \n",
            "have applications?\n",
            "Personal assistant, game,\n",
            "code generation, ‚Ä¶‚Ä¶\n",
            "(Knowledge) According to the previous works, large \n",
            "language model based agents refer to artificial ‚Ä¶‚Ä¶\n",
            "(Context) The current topic is LLM-based agent. ‚ÄúIt‚Äù \n",
            "refers to LLM-based agents in this conversation.\n",
            "Social Simulation\n",
            " I'm a compassionate physician \n",
            "specializing in cardiology, \n",
            "committed to improving patients' \n",
            "heart health and well-being.\n",
            "I'm a skilled nurse dedicated to \n",
            "patient care, ensuring comfort and \n",
            "supporting health with empathy \n",
            "and expertise.\n",
            "Role-playing\n",
            "I' m a Smurf, and \n",
            "Smurfs are us!\n",
            "Have you ever had \n",
            "a dream?\n",
            "Magic is all \n",
            "around the us!\n",
            "Jarvis, we must \n",
            "first learn to run!\n",
            "Wubalubadu.\n",
            "Bdub Wuckoop.\n",
            "I' m Batman, the \n",
            "lights of city.\n",
            "[Iron Man] My name is Iron Man, \n",
            "also known as Tony Stark. I am\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://newenglandhackopenai.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2025-03-01-preview \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://newenglandhackopenai.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2025-03-01-preview \"HTTP/1.1 200 OK\"\n",
            "INFO:mem0.vector_stores.mongodb:Vector search completed. Found 3 documents.\n",
            "INFO:mem0.vector_stores.mongodb:Vector search completed. Found 3 documents.\n",
            "INFO:mem0.memory.main:Total existing memories: 3\n",
            "INFO:httpx:HTTP Request: POST https://newenglandhackopenai.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2025-03-01-preview \"HTTP/1.1 200 OK\"\n",
            "INFO:mem0.memory.main:{'id': '0', 'text': 'Áî®Êà∑ÁöÑÂêçÂ≠óÊòØÂë®Êñå', 'event': 'UPDATE', 'old_memory': 'ÂêçÂ≠óÂè´Âë®Êñå'}\n",
            "INFO:mem0.memory.main:Updating memory with data='Áî®Êà∑ÁöÑÂêçÂ≠óÊòØÂë®Êñå'\n",
            "INFO:mem0.vector_stores.mongodb:Retrieved document with ID 'c7a1c439-a0c8-4674-be3b-4f90f4363990'.\n",
            "INFO:mem0.vector_stores.mongodb:Updated document with ID 'c7a1c439-a0c8-4674-be3b-4f90f4363990'.\n",
            "INFO:mem0.memory.main:Updating memory with ID memory_id='c7a1c439-a0c8-4674-be3b-4f90f4363990' with data='Áî®Êà∑ÁöÑÂêçÂ≠óÊòØÂë®Êñå'\n",
            "INFO:mem0.memory.main:{'id': '1', 'text': 'Áî®Êà∑ÁöÑÂêçÂ≠óÊòØuser001', 'event': 'DELETE'}\n",
            "INFO:mem0.memory.main:Deleting memory with memory_id='200770eb-98a6-403d-836b-8dcdb46c3f90'\n",
            "INFO:mem0.vector_stores.mongodb:Retrieved document with ID '200770eb-98a6-403d-836b-8dcdb46c3f90'.\n",
            "INFO:mem0.vector_stores.mongodb:Deleted document with ID '200770eb-98a6-403d-836b-8dcdb46c3f90'.\n",
            "INFO:mem0.memory.main:{'id': '2', 'text': 'Áî®Êà∑ÂñúÊ¨¢ÂêÉËãπÊûú', 'event': 'NONE'}\n",
            "INFO:mem0.memory.main:NOOP for Memory.\n",
            "INFO:mem0.vector_stores.mongodb:Retrieved document with ID 'c0c3c749-98eb-4d92-9620-8283735e0a66'.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'‰Ω†Âè´Âë®ÊñåÔºå‰Ω†ÂñúÊ¨¢ÂêÉËãπÊûú„ÄÇ'"
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chat_with_memories(\"ÊàëÂè´‰ªÄ‰πàÂêçÂ≠óÔºüÊàëÂñúÊ¨¢ÂêÉ‰ªÄ‰πàÊ∞¥ÊûúÔºü\",user_id=CURRENT_USER_ID)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv (3.12.12)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
