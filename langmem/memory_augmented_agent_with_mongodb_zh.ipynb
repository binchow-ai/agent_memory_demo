{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "afc42987",
      "metadata": {},
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/RichmondAlake/agent_memory_course/blob/main/langmem/memory_augmented_agent_with_mongodb.ipynb)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "0bcb92c2",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install -qU langmem langgraph langchain_voyageai langgraph-checkpoint-mongodb langgraph-store-mongodb pymongo requests pypdf langchain-community"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "d9b51274",
      "metadata": {},
      "outputs": [],
      "source": [
        "import getpass\n",
        "import os\n",
        "\n",
        "# å®‰å…¨åœ°è·å–å¹¶è®¾ç½®ç¯å¢ƒå˜é‡çš„å‡½æ•°\n",
        "def set_env_securely(var_name, prompt):\n",
        "    value = getpass.getpass(prompt)\n",
        "    os.environ[var_name] = value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "c887a4b8",
      "metadata": {},
      "outputs": [],
      "source": [
        "# set_env_securely(\"MONGODB_URI\", \"Enter your MongoDB URI: \")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "44ffb770",
      "metadata": {},
      "outputs": [],
      "source": [
        "# set_env_securely(\"VOYAGE_API_KEY\", \"Enter your Voyage API key: \")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "dc693319",
      "metadata": {},
      "outputs": [],
      "source": [
        "# set_env_securely(\"OPENAI_API_KEY\", \"Enter your OpenAI API Key: \")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "d04abc63",
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_openai import AzureChatOpenAI\n",
        "\n",
        "# ä½¿ç”¨ Azure æ—¶å»ºè®®å…³é—­ tracingï¼Œé¿å… 401ï¼ˆä¸ zero_to_hero ä¸­ä¸€è‡´ï¼‰\n",
        "os.environ[\"OPENAI_AGENTS_DISABLE_TRACING\"] = \"true\"\n",
        "\n",
        "azure_llm = AzureChatOpenAI(\n",
        "    azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),  # æˆ–ç›´æ¥å†™ \"https://xxx.openai.azure.com/\"\n",
        "    api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
        "    api_version=\"2025-03-01-preview\",  # æˆ– \"2024-12-01-preview\"\n",
        "    azure_deployment=\"gpt-4o\",          # ä½ çš„ deployment åç§°\n",
        "    model=\"gpt-4o\",                     # æ¨¡å‹ç±»å‹ï¼Œä¸ deployment å¯¹åº”\n",
        "    temperature=0,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "69efb4bc",
      "metadata": {},
      "outputs": [],
      "source": [
        "MONGODB_URI=os.environ[\"MONGODB_URI\"]\n",
        "DATABASE_NAME=\"langmem_agent_memory\"\n",
        "PROCEDURAL_MEMORY_COLLECTION_NAME=\"procedural_memory\"\n",
        "SEMANTIC_MEMORY_COLLECTION_NAME=\"semantic_memory\"\n",
        "STATE_CHECKPOINT_COLLECTION_NAME=\"state_checkpoints\"\n",
        "\n",
        "# ä¸ºä¸åŒç±»å‹çš„è®°å¿†å®šä¹‰å‘½åç©ºé—´\n",
        "USER_MEMORY_NAMESPACE = (\"user_memories\",)  # ç”¨æˆ·ä¸“å±è®°å¿†\n",
        "KNOWLEDGE_NAMESPACE = (\"agent_memory_survey\",)  # ç”¨äº PDF å†…å®¹ï¼ˆä¸æˆ‘ä»¬çš„å­˜å‚¨ä¸€è‡´ï¼‰"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "e6df36a3",
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_voyageai import VoyageAIEmbeddings\n",
        "from langgraph.store.mongodb.base import MongoDBStore, VectorIndexConfig\n",
        "from pymongo import MongoClient\n",
        "\n",
        "client = MongoClient(MONGODB_URI)\n",
        "db = client[DATABASE_NAME]\n",
        "procedural_collection = db[PROCEDURAL_MEMORY_COLLECTION_NAME]\n",
        "semantic_collection = db[SEMANTIC_MEMORY_COLLECTION_NAME]\n",
        "\n",
        "procedural_vector_index_config = VectorIndexConfig(\n",
        "    dims=1024,\n",
        "    index_name=\"procedural_memory_index\",\n",
        "    filters=None,\n",
        "    fields=None,\n",
        "    embed=VoyageAIEmbeddings(model=\"voyage-3-large\"),\n",
        ")\n",
        "\n",
        "semantic_vector_index_config = VectorIndexConfig(\n",
        "    dims=1024,\n",
        "    index_name=\"semantic_memory_index\",\n",
        "    filters=None,\n",
        "    fields=None,\n",
        "    embed=VoyageAIEmbeddings(model=\"voyage-3-large\"),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "a8387788",
      "metadata": {},
      "outputs": [],
      "source": [
        "procedural_memory_store = MongoDBStore(\n",
        "    collection=procedural_collection,\n",
        "    index_config=procedural_vector_index_config,\n",
        "    auto_index_timeout=70\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "ec885e1b",
      "metadata": {},
      "outputs": [],
      "source": [
        "semantic_memory_store = MongoDBStore(\n",
        "    collection=semantic_collection,\n",
        "    index_config=semantic_vector_index_config,\n",
        "    auto_index_timeout=70,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "22d8d308",
      "metadata": {},
      "outputs": [],
      "source": [
        "from langgraph.prebuilt import create_react_agent\n",
        "from langgraph.checkpoint.mongodb import MongoDBSaver\n",
        "\n",
        "checkpointer = MongoDBSaver(\n",
        "    client=client,\n",
        "    db_name=DATABASE_NAME, \n",
        "    collection_name=STATE_CHECKPOINT_COLLECTION_NAME,\n",
        "    index_config=procedural_memory_store\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "05819448",
      "metadata": {},
      "source": [
        "## æ•°æ®å¯¼å…¥ä¸é…ç½®"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0435957f",
      "metadata": {},
      "source": [
        "è¯­ä¹‰è®°å¿†å¯¼å…¥"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "8459bbc6",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading PDF from https://arxiv.org/pdf/2404.13501...\n",
            "Loading PDF with LangChain...\n",
            "Chunking text...\n",
            "Successfully created 198 chunks\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "\n",
        "# æ–¹æ³•ä¸€ï¼šå°†é¡¹ç›®æ ¹ç›®å½•åŠ å…¥ Python è·¯å¾„\n",
        "project_root = \"/Users/binzhou/Demo/agent_memory_demo/part1\"\n",
        "if project_root not in sys.path:\n",
        "    sys.path.insert(0, project_root)\n",
        "\n",
        "from utilities.pdf_chunker import ingest_pdf_and_chunk\n",
        "\n",
        "url = \"https://arxiv.org/pdf/2404.13501\"\n",
        "\n",
        "# å¯¼å…¥å¹¶åˆ†å— PDF\n",
        "chunks = ingest_pdf_and_chunk(url)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "93390b17",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Storing 198 chunks in procedural memory...\n",
            "All chunks stored successfully!\n",
            "\n",
            "First chunk example:\n",
            "Key: pdf_chunk_0\n",
            "Content preview: A Survey on the Memory Mechanism of Large\n",
            "Language Model based Agents\n",
            "Zeyu Zhang1, Xiaohe Bo1, Chen Ma1, Rui Li1, Xu Chen1, Quanyu Dai2,\n",
            "Jieming Zhu2, Zhenhua Dong2, Ji-Rong Wen1\n",
            "1Gaoling School of Ar...\n"
          ]
        }
      ],
      "source": [
        "from utilities.pdf_chunker import store_chunks_in_memory\n",
        "\n",
        "# å°†åˆ†å—å­˜å…¥è¯­ä¹‰è®°å¿†\n",
        "store_chunks_in_memory(chunks, semantic_memory_store, KNOWLEDGE_NAMESPACE[0])\n",
        "\n",
        "# æ‰“å°ç¬¬ä¸€ä¸ªåˆ†å—ä½œä¸ºç¤ºä¾‹\n",
        "if chunks:\n",
        "    print(\"\\nFirst chunk example:\")\n",
        "    print(f\"Key: {chunks[0]['key']}\")\n",
        "    print(f\"Content preview: {chunks[0]['value']['content'][:200]}...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "331b897c",
      "metadata": {},
      "outputs": [],
      "source": [
        "procedural_memory_store.put((\"instructions\",), key=\"agent_instructions\", value={\"prompt\": \"Write good paper summaries.\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "517899f5",
      "metadata": {},
      "outputs": [],
      "source": [
        "def prompt(state):\n",
        "    # è·å–æµç¨‹æŒ‡ä»¤\n",
        "    item = procedural_memory_store.get((\"instructions\",), key=\"agent_instructions\")\n",
        "    instructions = item.value[\"prompt\"]\n",
        "    \n",
        "    # è·å–ç”¨æˆ·æŸ¥è¯¢ä»¥è¿›è¡Œè¯­ä¹‰æ£€ç´¢\n",
        "    user_query = state[\"messages\"][-1].content\n",
        "    \n",
        "    # åœ¨è¯­ä¹‰è®°å¿†ä¸­æ£€ç´¢ç›¸å…³å†…å®¹\n",
        "    knowledge_items = []\n",
        "    try:\n",
        "        print(\"Searching semantic memory for relevant content\")\n",
        "        knowledge_items = semantic_memory_store.search(KNOWLEDGE_NAMESPACE, query=user_query, limit=5)\n",
        "        print(knowledge_items)\n",
        "    except Exception as e:\n",
        "        print(f\"Warning: Could not search semantic memory: {e}\")\n",
        "    \n",
        "    # ç”¨æŒ‡ä»¤å’Œç›¸å…³çŸ¥è¯†æ„å»ºç³»ç»Ÿå†…å®¹\n",
        "    system_content = f\"## Instructions\\n\\n{instructions}\\n\\n\"\n",
        "    \n",
        "    if knowledge_items:\n",
        "        system_content += \"## Relevant Knowledge from Agent Memory Research:\\n\"\n",
        "        for item in knowledge_items:\n",
        "            content = item.value.get('content', str(item.value))[:500]  # é™åˆ¶å†…å®¹é•¿åº¦\n",
        "            system_content += f\"- {content}...\\n\"\n",
        "        system_content += \"\\nUse this knowledge to provide informed, accurate responses.\\n\\n\"\n",
        "    \n",
        "    sys_prompt = {\"role\": \"system\", \"content\": system_content}\n",
        "    return [sys_prompt] + state['messages']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "840b6db4",
      "metadata": {},
      "outputs": [],
      "source": [
        "def simple_prompt(state):\n",
        "    \"\"\"ä»…åŒ…å«æµç¨‹æŒ‡ä»¤çš„ç®€å•æç¤º\"\"\"\n",
        "    item = procedural_memory_store.get((\"instructions\",), key=\"agent_instructions\")\n",
        "    instructions = item.value[\"prompt\"]\n",
        "    sys_prompt = {\"role\": \"system\", \"content\": f\"## Instructions\\n\\n{instructions}\\n\\nYou have access to search tools for knowledge retrieval when needed.\"}\n",
        "    return [sys_prompt] + state['messages']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "8004e5e0",
      "metadata": {},
      "outputs": [],
      "source": [
        "from langmem import create_manage_memory_tool, create_search_memory_tool\n",
        "\n",
        "# ğŸš¨ tools å‚æ•°å¯è½¬ä¸ºå·¥å…·ç®±è®°å¿†ï¼ˆä¸€ç§æµç¨‹è®°å¿†ï¼Œç”¨äºå­˜å‚¨å·¥å…·åŠå…¶æè¿°ï¼Œå¯ä½¿ç”¨ BigToolsï¼‰\n",
        "# ä¸ºä¸åŒå‘½åç©ºé—´åˆ›å»ºè®°å¿†ç®¡ç†å·¥å…·\n",
        "memory_tools = [\n",
        "    create_manage_memory_tool(USER_MEMORY_NAMESPACE),    # ç”¨æˆ·ä¸“å±è®°å¿†\n",
        "    create_search_memory_tool(USER_MEMORY_NAMESPACE),    # æ£€ç´¢ç”¨æˆ·è®°å¿†\n",
        "    create_search_memory_tool(KNOWLEDGE_NAMESPACE),      # æ£€ç´¢ PDF çŸ¥è¯†åº“\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8dfd8cb2",
      "metadata": {},
      "source": [
        "ä½¿ç”¨è®°å¿†å·¥å…·çš„æ™ºèƒ½ä½“ "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "d119a6b3",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/var/folders/0c/6xdkz3fs2m931gvzk6gz44mh0000gp/T/ipykernel_28142/3022040874.py:1: LangGraphDeprecatedSinceV10: create_react_agent has been moved to `langchain.agents`. Please update your import to `from langchain.agents import create_agent`. Deprecated in LangGraph V1.0 to be removed in V2.0.\n",
            "  agent = create_react_agent(\n"
          ]
        }
      ],
      "source": [
        "agent = create_react_agent(\n",
        "    azure_llm,\n",
        "    prompt=prompt,  # ä»æ•°æ®åº“ï¼ˆæµç¨‹è®°å¿†ï¼‰è·å–çš„æ™ºèƒ½ä½“æç¤º\n",
        "    tools=memory_tools,\n",
        "    store= procedural_memory_store,  # å­˜å‚¨è¯­ä¹‰çŸ¥è¯†\n",
        "    checkpointer=checkpointer,  # åœ¨æ•°æ®åº“ä¸­å­˜å‚¨æ™ºèƒ½ä½“çŠ¶æ€ï¼ˆæµç¨‹è®°å¿†ï¼‰\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "233fca87",
      "metadata": {},
      "outputs": [],
      "source": [
        "def chat_with_agent(agent, query, thread_id, user_id=None, return_full_result=False):\n",
        "    \"\"\"\n",
        "    å¢å¼ºç‰ˆå¯¹è¯å‡½æ•°ï¼šæ”¯æŒç”¨æˆ·ä¸“å±è®°å¿†ï¼Œå¹¶å¯è¿”å›å®Œæ•´ç»“æœã€‚\n",
        "\n",
        "    å‚æ•°:\n",
        "        agent: è¦è°ƒç”¨çš„æ™ºèƒ½ä½“\n",
        "        query: ç”¨æˆ·æ¶ˆæ¯æ–‡æœ¬\n",
        "        thread_id: ä¼šè¯çº¿ç¨‹ IDï¼Œç”¨äºä¿æŒå¯¹è¯è¿ç»­æ€§\n",
        "        user_id: å¯é€‰ç”¨æˆ· IDï¼Œç”¨äºä¸ªæ€§åŒ–è®°å¿†\n",
        "        return_full_result: ä¸º True æ—¶è¿”å›å®Œæ•´ç»“æœçŠ¶æ€ï¼›ä¸º False æ—¶ä»…è¿”å›æœ€åä¸€æ¡æ¶ˆæ¯å†…å®¹\n",
        "\n",
        "    è¿”å›:\n",
        "        return_full_result=True æ—¶ï¼šåŒ…å«æ‰€æœ‰æ¶ˆæ¯çš„å®Œæ•´ç»“æœçŠ¶æ€\n",
        "        return_full_result=False æ—¶ï¼šä»…æœ€åä¸€æ¡æ¶ˆæ¯çš„å†…å®¹\n",
        "    \"\"\"\n",
        "    config = {\"configurable\": {\"thread_id\": thread_id}}\n",
        "    if user_id:\n",
        "        config[\"configurable\"][\"user_id\"] = user_id\n",
        "    \n",
        "    result_state = agent.invoke(\n",
        "        {\"messages\": [{\"role\": \"user\", \"content\": query}]}, \n",
        "        config=config\n",
        "    )\n",
        "    \n",
        "    if return_full_result:\n",
        "        return result_state\n",
        "    else:\n",
        "        return result_state[\"messages\"][-1].content\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "9801c541",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Searching semantic memory for relevant content\n",
            "[Item(namespace=['agent_memory_survey'], key='pdf_chunk_0', value={'content': 'A Survey on the Memory Mechanism of Large\\nLanguage Model based Agents\\nZeyu Zhang1, Xiaohe Bo1, Chen Ma1, Rui Li1, Xu Chen1, Quanyu Dai2,\\nJieming Zhu2, Zhenhua Dong2, Ji-Rong Wen1\\n1Gaoling School of Artificial Intelligence, Renmin University of China, Beijing, China\\n2Huawei Noahâ€™s Ark Lab, China\\nzeyuzhang@ruc.edu.cn, xu.chen@ruc.edu.cn\\nAbstract\\nLarge language model (LLM) based agents have recently attracted much attention\\nfrom the research and industry communities. Compared with original LLMs, LLM-\\nbased agents are featured in their self-evolving capability, which is the basis for\\nsolving real-world problems that need long-term and complex agent-environment\\ninteractions. The key component to support agent-environment interactions is the\\nmemory of the agents. While previous studies have proposed many promising mem-\\nory mechanisms, they are scattered in different papers, and there lacks a systemati-\\ncal review to summarize and compare these works from a holistic perspective, fail-', 'source': 'https://arxiv.org/pdf/2404.13501', 'chunk_index': 0, 'total_chunks': 198}, created_at='2026-02-03T05:18:31.186000', updated_at='2026-02-03T06:49:47.962000', score=0.874698281288147), Item(namespace=['agent_memory_survey'], key='pdf_chunk_147', value={'content': '[4] Zhiheng Xi, Wenxiang Chen, Xin Guo, Wei He, Yiwen Ding, Boyang Hong, Ming Zhang,\\nJunzhe Wang, Senjie Jin, Enyu Zhou, et al. The rise and potential of large language model\\nbased agents: A survey. arXiv preprint arXiv:2309.07864, 2023.\\n[5] Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik R Narasimhan, and Shunyu Yao.\\nReflexion: Language agents with verbal reinforcement learning. In Thirty-seventh Conference\\non Neural Information Processing Systems, 2023.\\n[6] Wanjun Zhong, Lianghong Guo, Qiqi Gao, and Yanlin Wang. Memorybank: Enhancing large\\nlanguage models with long-term memory. arXiv preprint arXiv:2305.10250, 2023.\\n[7] Ali Modarressi, Ayyoob Imani, Mohsen Fayyaz, and Hinrich SchÃ¼tze. Ret-llm: Towards\\na general read-write memory for large language models. arXiv preprint arXiv:2305.14322,\\n2023.\\n[8] Shengyu Zhang, Linfeng Dong, Xiaoya Li, Sen Zhang, Xiaofei Sun, Shuhe Wang, Jiwei Li,\\nRunyi Hu, Tianwei Zhang, Fei Wu, et al. Instruction tuning for large language models: A', 'source': 'https://arxiv.org/pdf/2404.13501', 'chunk_index': 147, 'total_chunks': 198}, created_at='2026-02-03T05:19:50.682000', updated_at='2026-02-03T06:51:05.680000', score=0.8639869689941406), Item(namespace=['agent_memory_survey'], key='pdf_chunk_174', value={'content': '[96] Chenxu Hu, Jie Fu, Chenzhuang Du, Simian Luo, Junbo Zhao, and Hang Zhao. Chatdb:\\nAugmenting llms with databases as their symbolic memory. arXiv preprint arXiv:2306.03901,\\n2023.\\n[97] Lei Liu, Xiaoyan Yang, Yue Shen, Binbin Hu, Zhiqiang Zhang, Jinjie Gu, and Guannan Zhang.\\nThink-in-memory: Recalling and post-thinking enable llms with long-term memory. arXiv\\npreprint arXiv:2311.08719, 2023.\\n[98] Xinnian Liang, Bing Wang, Hui Huang, Shuangzhi Wu, Peihao Wu, Lu Lu, Zejun Ma, and\\nZhoujun Li. Unleashing infinite-length input capacity for large-scale language models with\\nself-controlled memory system. arXiv preprint arXiv:2304.13343, 2023.\\n[99] Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi\\nFan, and Anima Anandkumar. V oyager: An open-ended embodied agent with large language\\nmodels. arXiv preprint arXiv:2305.16291, 2023.\\n[100] Charles Packer, Vivian Fang, Shishir G Patil, Kevin Lin, Sarah Wooders, and Joseph E', 'source': 'https://arxiv.org/pdf/2404.13501', 'chunk_index': 174, 'total_chunks': 198}, created_at='2026-02-03T05:20:04.892000', updated_at='2026-02-03T06:51:20.010000', score=0.8597463369369507), Item(namespace=['agent_memory_survey'], key='pdf_chunk_19', value={'content': 'ory module and comprehensively analyze its necessity for LLM-based agents. (2) We systematically\\nsummarize existing studies on designing and evaluating the memory module in LLM-based agents,\\nproviding clear taxonomies and intuitive insights. (3) We present typical agent applications to show\\nthe importance of the memory module in different scenarios. (4) We analyze the key limitations of\\nexisting memory modules and show potential solutions for inspiring future studies. To our knowledge,\\nthis is the first survey on the memory mechanism of LLM-based agents.\\nThe rest of this survey is organized as follows. First, we provide a systematical meta-survey for the\\nfields of LLMs and LLM-based agents in Section 2, categorizing different surveys and summarizing\\ntheir key contributions. Then, we discuss the problems of â€œwhat isâ€, â€œwhy do we needâ€ and â€œhow\\nto implement and evaluateâ€ the memory module in LLM-based agents inSection 3 to 6. Next, we', 'source': 'https://arxiv.org/pdf/2404.13501', 'chunk_index': 19, 'total_chunks': 198}, created_at='2026-02-03T05:18:41.179000', updated_at='2026-02-03T06:49:57.884000', score=0.8517197370529175), Item(namespace=['agent_memory_survey'], key='pdf_chunk_8', value={'content': 'Contents\\n1 Introduction 4\\n2 Related Surveys 5\\n2.1 Surveys on Large Language Models . . . . . . . . . . . . . . . . . . . . . . . . . 5\\n2.2 Surveys on Large Language Model-based Agents . . . . . . . . . . . . . . . . . . 7\\n3 What is the Memory of LLM-based Agent 7\\n3.1 Basic Knowledge . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7\\n3.2 Narrow Definition of the Agent Memory . . . . . . . . . . . . . . . . . . . . . . . 9\\n3.3 Broad Definition of the Agent Memory . . . . . . . . . . . . . . . . . . . . . . . . 9\\n3.4 Memory-assisted Agent-Environment Interaction . . . . . . . . . . . . . . . . . . 9\\n4 Why We Need the Memory in LLM-based Agent 10\\n4.1 Perspective of Cognitive Psychology . . . . . . . . . . . . . . . . . . . . . . . . . 10\\n4.2 Perspective of Self-Evolution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11\\n4.3 Perspective of Agent Applications . . . . . . . . . . . . . . . . . . . . . . . . . . 11', 'source': 'https://arxiv.org/pdf/2404.13501', 'chunk_index': 8, 'total_chunks': 198}, created_at='2026-02-03T05:18:35.161000', updated_at='2026-02-03T06:49:52.111000', score=0.8506951332092285)]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "response = chat_with_agent(\n",
        "    agent=agent, \n",
        "    query=\"Who are the authors of the paper A survey on the memory mechanism of large language models?\",\n",
        "    thread_id=\"10\",\n",
        "    user_id=\"user-123\",\n",
        "    return_full_result=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "061b6d6d",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The authors of the paper \"A Survey on the Memory Mechanism of Large Language Models\" are:\n",
            "\n",
            "- **Zeyu Zhang**\n",
            "- **Xiaohe Bo**\n",
            "- **Chen Ma**\n",
            "- **Rui Li**\n",
            "- **Xu Chen**\n",
            "- **Quanyu Dai**\n",
            "- **Jieming Zhu**\n",
            "- **Zhenhua Dong**\n",
            "- **Ji-Rong Wen**\n",
            "\n",
            "They are affiliated with the Gaoling School of Artificial Intelligence, Renmin University of China, and Huawei Noahâ€™s Ark Lab, China.\n"
          ]
        }
      ],
      "source": [
        "print(response['messages'][-1].content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "id": "134d6f16",
      "metadata": {},
      "outputs": [],
      "source": [
        "from langmem import create_prompt_optimizer\n",
        "\n",
        "optimizer = create_prompt_optimizer(azure_llm)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1a2f86fd",
      "metadata": {},
      "source": [
        "ç†è§£å¹¶è§£é‡Šè½¨è¿¹"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "id": "5800a21b",
      "metadata": {},
      "outputs": [],
      "source": [
        "current_prompt = procedural_memory_store.get((\"instructions\",), key=\"agent_instructions\").value[\"prompt\"]\n",
        "feedback = {\"request\": \"Always respond with sentences starting with, according to the paper in question...\"}\n",
        "\n",
        "optimizer_result = optimizer.invoke({\"prompt\": current_prompt, \"trajectories\": [(response[\"messages\"], feedback)]})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "id": "51ce5ad3",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Current prompt: Write good paper summaries.\n",
            "Optimizer result: Write good paper summaries. Always begin responses with the phrase: 'According to the paper in question...' Ensure consistency in factual information by cross-referencing multiple sources or explicitly stating when there is conflicting information. If uncertain about any information, include a fallback statement such as: 'There appear to be conflicting sources regarding the authors of this paper.'\n"
          ]
        }
      ],
      "source": [
        "print(f\"Current prompt: {current_prompt}\")\n",
        "print(f\"Optimizer result: {optimizer_result}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "id": "f5c2c8b3",
      "metadata": {},
      "outputs": [],
      "source": [
        "procedural_memory_store.put((\"instructions\",), key=\"agent_instructions\", value={\"prompt\": optimizer_result})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "id": "a6ff1efb",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Searching semantic memory for relevant content\n",
            "[Item(namespace=['agent_memory_survey'], key='pdf_chunk_22', value={'content': 'which, however, provide different taxonomies and understandings on LLMs. Following these surveys,\\npeople dive into specific aspects of LLMs and review the corresponding milestone studies and key\\ntechnologies. These aspects can be classified into four categories including the fundamental problems,\\nevaluation, applications, and challenges of LLMs.\\nFundamental problems. The surveys in this category aim to summarize techniques that can\\nbe leveraged to tackle fundamental problems of LLMs. Specifically, Zhang et al. [8] provide a\\ncomprehensive survey on the methods of supervised fine-tuning, which is a key technique for better\\ntraining LLMs. Shen et al. [9], Wang et al.[10] and Liu et al. [11] present surveys on the alignment of\\nLLMs, which is a key requirement for LLMs to produce outputs consistent with human values. Gao\\net al. [12] propose a survey on the retrieval-augmented generation (RAG) capability of LLMs, which', 'source': 'https://arxiv.org/pdf/2404.13501', 'chunk_index': 22, 'total_chunks': 198}, created_at='2026-02-03T05:18:42.718000', updated_at='2026-02-03T06:49:59.520000', score=0.7372990250587463), Item(namespace=['agent_memory_survey'], key='pdf_chunk_20', value={'content': 'their key contributions. Then, we discuss the problems of â€œwhat isâ€, â€œwhy do we needâ€ and â€œhow\\nto implement and evaluateâ€ the memory module in LLM-based agents inSection 3 to 6. Next, we\\nshow the applications of memory-enhanced agents in Section 7. The discussions of the limitations of\\nexisting work and future directions come at last in Section 8 and Section 9.\\n4', 'source': 'https://arxiv.org/pdf/2404.13501', 'chunk_index': 20, 'total_chunks': 198}, created_at='2026-02-03T05:18:41.675000', updated_at='2026-02-03T06:49:58.437000', score=0.7371910214424133), Item(namespace=['agent_memory_survey'], key='pdf_chunk_83', value={'content': 'The fine-tuning methods can effectively bridge the gap between general agents and specialized\\nagents. It improves the capability of agents on the tasks that require high accuracy and reliability on\\ndomain-specific information. Nevertheless, fine-tuning LLMs for specific domains could potentially\\nlead to overfitting, and it also raises concerns about catastrophic forgetting, where LLMs may forget\\nthe original knowledge because of updating their parameters. Another limitation of fine-tuning lies\\nin the computational cost and time consumption, as well as the requirement of a large amount of\\ndata. Therefore, most fine-tuning approaches are applied to offline scenarios, and can seldom deal\\nwith online scenarios, such as fine-tuning with agent observations and trial experiences. Due to\\nthe frequent agent-environment interactions, it is unaffordable for the cost of backpropagation to\\nfine-tune every step of the online and dynamic interactions.', 'source': 'https://arxiv.org/pdf/2404.13501', 'chunk_index': 83, 'total_chunks': 198}, created_at='2026-02-03T05:19:14.942000', updated_at='2026-02-03T06:50:31.439000', score=0.7348092198371887), Item(namespace=['agent_memory_survey'], key='pdf_chunk_16', value={'content': 'informative knowledge to support its actions, and so on. Around the memory module, people have\\ndevoted much effort to designing its information sources, storage forms, and operation mechanisms.\\nFor example, Shinn et al. [5] incorporate both in-trial and cross-trial information to build the memory\\nmodule for enhancing the agentâ€™s reasoning capability. Zhong et al.[6] store memory information in\\nthe form of natural languages, which is explainable and friendly to the users. Modarressi et al. [7]\\ndesign both memory reading and writing operations to interact with environments for task solving.\\nWhile previous studies have designed many promising memory modules, there still lacks a systemic\\nstudy to view the memory modules from a holistic perspective. To bridge this gap, in this paper,\\nwe comprehensively review previous studies to present clear taxonomies and key principles for\\ndesigning and evaluating the memory module. In specific, we discuss three key problems including:', 'source': 'https://arxiv.org/pdf/2404.13501', 'chunk_index': 16, 'total_chunks': 198}, created_at='2026-02-03T05:18:39.629000', updated_at='2026-02-03T06:49:56.318000', score=0.733630895614624), Item(namespace=['agent_memory_survey'], key='pdf_chunk_50', value={'content': 'Cognitive psychology is the scientific study of human mental processes such as attention, language\\nuse, memory, perception, problem-solving, creativity, and reasoning 2. Among these processes,\\nmemory is widely recognized as an extremely important one [84]. It is fundamental for humans to\\nlearn knowledge by accumulating important information and abstracting high-level concepts [85],\\nform social norms by remembering cultural values and individual experiences [86], take reasonable\\nbehaviors by imagining the potential positive and negative consequences [87], and among others.\\nA major goal of LLM-based agents is to replace humans for accomplishing different tasks. To make\\nagents behave like humans, following humanâ€™s working mechanisms to design the agents is a natural\\nand essential choice [88]. Since memory is important for humans, designing memory modules is also\\nsignificant for the agents. In addition, cognitive psychology has been studied for a long time, so many', 'source': 'https://arxiv.org/pdf/2404.13501', 'chunk_index': 50, 'total_chunks': 198}, created_at='2026-02-03T05:18:57.437000', updated_at='2026-02-03T06:50:14.132000', score=0.7313448190689087)]\n"
          ]
        }
      ],
      "source": [
        "response = chat_with_agent(\n",
        "    agent,\n",
        "    \"What is the main point of the paper?\",\n",
        "    \"0\",\n",
        "    \"user-123\",\n",
        "    return_full_result=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "id": "3dbdbe8b",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "According to the paper in question, the main point appears to focus on the exploration of large language models (LLMs), their fundamental problems, evaluation methods, applications, and challenges. It delves into the design and implementation of memory modules in LLM-based agents, discussing their importance in enhancing reasoning capabilities, storing information in user-friendly formats, and addressing issues like catastrophic forgetting during fine-tuning. Additionally, the paper highlights the applications of memory-enhanced agents and discusses their limitations and future directions. If you have a specific paper in mind, please provide more details for a precise summary.\n"
          ]
        }
      ],
      "source": [
        "print(response['messages'][-1].content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "id": "8974dd9e",
      "metadata": {},
      "outputs": [],
      "source": [
        "def eager_prompt(state):\n",
        "    \"\"\"ä½¿ç”¨ä¸»åŠ¨æ£€ç´¢çš„æç¤ºï¼ˆæ— éœ€ store å‚æ•°ï¼‰\"\"\"\n",
        "    # è·å–æµç¨‹æŒ‡ä»¤\n",
        "    item = procedural_memory_store.get((\"instructions\",), key=\"agent_instructions\")\n",
        "    instructions = item.value[\"prompt\"]\n",
        "    \n",
        "    # è·å–ç”¨æˆ·æŸ¥è¯¢ä»¥è¿›è¡Œè¯­ä¹‰æ£€ç´¢\n",
        "    user_query = state[\"messages\"][-1].content\n",
        "    \n",
        "    # ç›´æ¥è®¿é—®è¯­ä¹‰è®°å¿†ï¼ˆæ— éœ€ store å‚æ•°ï¼‰\n",
        "    knowledge_items = []\n",
        "    try:\n",
        "        knowledge_items = semantic_memory_store.search(KNOWLEDGE_NAMESPACE, query=user_query, limit=3)\n",
        "    except Exception as e:\n",
        "        print(f\"Warning: Could not search semantic memory: {e}\")\n",
        "    \n",
        "    # æ„å»ºç³»ç»Ÿå†…å®¹\n",
        "    system_content = f\"## Instructions\\n\\n{instructions}\\n\\n\"\n",
        "    \n",
        "    if knowledge_items:\n",
        "        system_content += \"## Relevant Knowledge:\\n\"\n",
        "        for item in knowledge_items:\n",
        "            content = item.value.get('content', str(item.value))[:400]\n",
        "            system_content += f\"- {content}...\\n\"\n",
        "        system_content += \"\\n\"\n",
        "    \n",
        "    sys_prompt = {\"role\": \"system\", \"content\": system_content}\n",
        "    return [sys_prompt] + state['messages']\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "id": "e9eef7ce",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/var/folders/0c/6xdkz3fs2m931gvzk6gz44mh0000gp/T/ipykernel_28142/3265235424.py:2: LangGraphDeprecatedSinceV10: create_react_agent has been moved to `langchain.agents`. Please update your import to `from langchain.agents import create_agent`. Deprecated in LangGraph V1.0 to be removed in V2.0.\n",
            "  eager_agent = create_react_agent(\n"
          ]
        }
      ],
      "source": [
        "# åˆ›å»ºä½¿ç”¨ä¸»åŠ¨æ£€ç´¢çš„æ™ºèƒ½ä½“ï¼ˆè¯­ä¹‰è®°å¿†æ— éœ€ store å‚æ•°ï¼‰\n",
        "eager_agent = create_react_agent(\n",
        "    azure_llm,\n",
        "    prompt=eager_prompt,  # è‡ªåŠ¨æ³¨å…¥çŸ¥è¯†\n",
        "    tools=[],  # æ— éœ€æ£€ç´¢å·¥å…·\n",
        "    store=procedural_memory_store,  # store ä¸­ä»…å«æµç¨‹è®°å¿†\n",
        "    checkpointer=checkpointer,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "id": "ff6135e4",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Approach 2: Eager Retrieval (Always includes relevant knowledge) ===\n",
            "Response: According to the paper in question, there appear to be conflicting sources regarding the authors of this paper. If the paper is titled \"Communicative agents for software development,\" the authors are ...\n"
          ]
        }
      ],
      "source": [
        "\n",
        "print(\"\\n=== Approach 2: Eager Retrieval (Always includes relevant knowledge) ===\")\n",
        "query = \"Who are the authors of the paper?\"\n",
        "response2 = chat_with_agent(eager_agent, query, \"eager-test\", return_full_result=False)\n",
        "print(f\"Response: {response2[:200]}...\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "id": "381b050f",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "# å®‰è£… BigTool\n",
        "%pip install -q langgraph-bigtool\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "id": "89fce5a6",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Created 12 memory tools:\n",
            "  - search_user_memories: Search store and retrieve user-specific information, preferences, and personal data\n",
            "  - manage_user_memories: Store/update information in store and retrieve user-specific information, preferences, and personal data\n",
            "  - search_agent_memory_survey: Search search the comprehensive research paper about agent memory mechanisms\n",
            "  - manage_agent_memory_survey: Store/update information in search the comprehensive research paper about agent memory mechanisms\n",
            "  - search_conversations: Search store and retrieve conversation history and context\n",
            "  - manage_conversations: Store/update information in store and retrieve conversation history and context\n",
            "  - search_user_preferences: Search manage user settings, preferences, and configuration\n",
            "  - manage_user_preferences: Store/update information in manage user settings, preferences, and configuration\n",
            "  - search_project_knowledge: Search store and retrieve project-specific knowledge and documentation\n",
            "  - manage_project_knowledge: Store/update information in store and retrieve project-specific knowledge and documentation\n",
            "  - search_research_papers: Search general research paper storage and retrieval\n",
            "  - manage_research_papers: Store/update information in general research paper storage and retrieval\n"
          ]
        }
      ],
      "source": [
        "import uuid\n",
        "from typing import Literal, Dict, Any\n",
        "from langmem import create_manage_memory_tool, create_search_memory_tool\n",
        "from langgraph_bigtool import create_agent\n",
        "from langgraph.prebuilt import InjectedStore\n",
        "from langgraph.store.base import BaseStore\n",
        "from typing_extensions import Annotated\n",
        "\n",
        "# å®šä¹‰å¯ç”¨çš„è®°å¿†å‘½åç©ºé—´åŠå…¶ç”¨é€”\n",
        "MEMORY_NAMESPACES = {\n",
        "    \"user_memories\": \"å­˜å‚¨ä¸æ£€ç´¢ç”¨æˆ·ä¸“å±ä¿¡æ¯ã€åå¥½åŠä¸ªäººæ•°æ®\",\n",
        "    \"agent_memory_survey\": \"æ£€ç´¢å…³äºæ™ºèƒ½ä½“è®°å¿†æœºåˆ¶çš„ç»¼åˆç ”ç©¶è®ºæ–‡\",\n",
        "    \"conversations\": \"å­˜å‚¨ä¸æ£€ç´¢å¯¹è¯å†å²ä¸ä¸Šä¸‹æ–‡\",\n",
        "    \"user_preferences\": \"ç®¡ç†ç”¨æˆ·è®¾ç½®ã€åå¥½ä¸é…ç½®\",\n",
        "    \"project_knowledge\": \"å­˜å‚¨ä¸æ£€ç´¢é¡¹ç›®ä¸“å±çŸ¥è¯†ä¸æ–‡æ¡£\",\n",
        "    \"research_papers\": \"é€šç”¨ç ”ç©¶è®ºæ–‡çš„å­˜å‚¨ä¸æ£€ç´¢\",\n",
        "}\n",
        "\n",
        "def create_dynamic_memory_tool_registry() -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    åˆ›å»ºå¯åŠ¨æ€æ£€ç´¢çš„è®°å¿†å·¥å…·æ³¨å†Œè¡¨ã€‚\n",
        "    ç›¸æ¯”å°†æ‰€æœ‰å·¥å…·æ”¾åœ¨æ•°ç»„ä¸­ï¼Œè¿™ç§æ–¹å¼æ›´å…·å¯æ‰©å±•æ€§ã€‚\n",
        "    \"\"\"\n",
        "    tool_registry = {}\n",
        "    \n",
        "    # ä¸ºæ¯ä¸ªå‘½åç©ºé—´åˆ›å»ºæ£€ç´¢å’Œç®¡ç†å·¥å…·\n",
        "    for namespace, description in MEMORY_NAMESPACES.items():\n",
        "        namespace_tuple = (namespace,)\n",
        "        \n",
        "        # è¯¥å‘½åç©ºé—´çš„æ£€ç´¢å·¥å…·\n",
        "        search_tool = create_search_memory_tool(namespace_tuple)\n",
        "        search_tool.description = f\"Search {description.lower()}\"\n",
        "        search_id = f\"search_{namespace}\"\n",
        "        tool_registry[search_id] = search_tool\n",
        "        \n",
        "        # è¯¥å‘½åç©ºé—´çš„ç®¡ç†å·¥å…·\n",
        "        manage_tool = create_manage_memory_tool(namespace_tuple)\n",
        "        manage_tool.description = f\"Store/update information in {description.lower()}\"\n",
        "        manage_id = f\"manage_{namespace}\"\n",
        "        tool_registry[manage_id] = manage_tool\n",
        "    \n",
        "    return tool_registry\n",
        "\n",
        "# åˆ›å»ºå·¥å…·æ³¨å†Œè¡¨\n",
        "memory_tool_registry = create_dynamic_memory_tool_registry()\n",
        "\n",
        "print(f\"Created {len(memory_tool_registry)} memory tools:\")\n",
        "for tool_id, tool in memory_tool_registry.items():\n",
        "    print(f\"  - {tool_id}: {tool.description}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "id": "5c1f3c35",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Memory tools indexed for semantic retrieval\n"
          ]
        }
      ],
      "source": [
        "# åœ¨ store ä¸­ä¸ºè¯­ä¹‰æ£€ç´¢å»ºç«‹è®°å¿†å·¥å…·ç´¢å¼•\n",
        "from langchain_voyageai import VoyageAIEmbeddings\n",
        "\n",
        "# ä¸ºå·¥å…·ç´¢å¼•åˆ›å»ºç‹¬ç«‹çš„ store\n",
        "tool_store = MongoDBStore(\n",
        "    collection=db[\"memory_tools\"],\n",
        "    index_config=VectorIndexConfig(\n",
        "        dims=1024,\n",
        "        index_name=\"memory_tools_index\", \n",
        "        embed=VoyageAIEmbeddings(model=\"voyage-3-large\"),\n",
        "        filters=None,\n",
        "        fields=None,\n",
        "        auto_index_timeout=70\n",
        "    )\n",
        ")\n",
        "\n",
        "# ç”¨æ¯ä¸ªå·¥å…·çš„æè¿°å»ºç«‹ç´¢å¼•ä»¥æ”¯æŒè¯­ä¹‰æ£€ç´¢\n",
        "for tool_id, tool in memory_tool_registry.items():\n",
        "    tool_store.put(\n",
        "        (\"memory_tools\",),\n",
        "        tool_id,\n",
        "        {\n",
        "            \"name\": tool.name,\n",
        "            \"description\": tool.description,\n",
        "            \"namespace\": tool_id.split(\"_\", 1)[1],  # ä» ID æå–å‘½åç©ºé—´\n",
        "            \"operation\": tool_id.split(\"_\", 1)[0],  # æå–æ“ä½œç±»å‹ï¼ˆsearch/manageï¼‰\n",
        "        },\n",
        "    )\n",
        "\n",
        "print(\"Memory tools indexed for semantic retrieval\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "id": "cdb6ce14",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Custom tool retrieval function created\n"
          ]
        }
      ],
      "source": [
        "def retrieve_memory_tools(\n",
        "    query: str,\n",
        "    operation_type: Literal[\"search\", \"manage\", \"both\"] = \"both\",\n",
        "    *,\n",
        "    store: Annotated[BaseStore, InjectedStore],\n",
        ") -> list[str]:\n",
        "    \"\"\"\n",
        "    æ ¹æ®æŸ¥è¯¢ä¸æ“ä½œç±»å‹æ™ºèƒ½æ£€ç´¢è®°å¿†å·¥å…·ã€‚\n",
        "\n",
        "    å‚æ•°:\n",
        "        query: ç”¨æˆ·æŸ¥è¯¢ï¼Œç”¨äºç¡®å®šç›¸å…³è®°å¿†å‘½åç©ºé—´\n",
        "        operation_type: è¦è¿”å›æ£€ç´¢ç±»å·¥å…·ã€ç®¡ç†ç±»å·¥å…·æˆ–ä¸¤è€…\n",
        "        store: æ³¨å…¥çš„å·¥å…· storeï¼Œç”¨äºæœç´¢\n",
        "\n",
        "    è¿”å›:\n",
        "        ä¸æŸ¥è¯¢ç›¸å…³çš„å·¥å…· ID åˆ—è¡¨\n",
        "    \"\"\"\n",
        "    # æ ¹æ®æŸ¥è¯¢æ£€ç´¢ç›¸å…³å·¥å…·\n",
        "    results = store.search((\"memory_tools\",), query=query, limit=6)\n",
        "    \n",
        "    # æŒ‰æ“ä½œç±»å‹è¿‡æ»¤\n",
        "    tool_ids = []\n",
        "    for result in results:\n",
        "        tool_id = result.key\n",
        "        tool_data = result.value\n",
        "        \n",
        "        if operation_type == \"both\":\n",
        "            tool_ids.append(tool_id)\n",
        "        elif operation_type == \"search\" and tool_data[\"operation\"] == \"search\":\n",
        "            tool_ids.append(tool_id)\n",
        "        elif operation_type == \"manage\" and tool_data[\"operation\"] == \"manage\":\n",
        "            tool_ids.append(tool_id)\n",
        "    \n",
        "    # å¯¹ç ”ç©¶ç±»é—®é¢˜ç¡®ä¿åŒ…å«ç ”ç©¶è®ºæ–‡æ£€ç´¢å·¥å…·\n",
        "    research_indicators = [\"paper\", \"research\", \"author\", \"study\", \"survey\", \"memory mechanism\"]\n",
        "    if any(indicator in query.lower() for indicator in research_indicators):\n",
        "        if \"search_agent_memory_survey\" not in tool_ids:\n",
        "            tool_ids.insert(0, \"search_agent_memory_survey\")\n",
        "    \n",
        "    # é™åˆ¶æ•°é‡ä»¥å…ç»™ LLM è¿‡å¤šå·¥å…·\n",
        "    return tool_ids[:4]\n",
        "\n",
        "print(\"Custom tool retrieval function created\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv (3.12.12)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
